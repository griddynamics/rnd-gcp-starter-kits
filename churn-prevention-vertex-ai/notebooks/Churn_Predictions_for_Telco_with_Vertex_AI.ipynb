{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "peimT_FWC7TF"
      },
      "source": [
        "# Churn Predictions for Telco company Using Google Vertex AI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ot4PLXgvB1t"
      },
      "source": [
        "This notebook represents a starter kit for implementing churn analytic pipeline with autoML provided by Vertex AI. In other words, it shows best practices of how to implement auto ml capabilities in modeling churn predictions, and how to utilise those models for gaining better insights when it comes to reducing number of churned users. Considering all above, we included following steps in the notebook:\n",
        "\n",
        "1. Initialization and authentification\n",
        "2. Usuful functions\n",
        "2. Load data\n",
        "3. Short-term churn prediction - model used for predicting churns for short terms time horizon, such as prediction for next month. In general, it shows **WHO** is likely going to churn. This type of models, used the most recent behavioral data to predict next user event.\n",
        "4. Long-term churn predictor - model is used to revield **WHEN** users have more chance to churn, and it helps business retaining them on a time. This model use sequence of events from recent users past in order to predict their future behaviour, usually inside predefined future time horizont.\n",
        "5. Uplift modeling for selecting treatment - model that shows **HOW** we may prevent users from churning with a best treatments suited for them. Uplift models help buisness in matching users with the right treatment and it could be used in process of optimisation of costs.\n",
        "6. Identifiaction of triggers - it shows **WHAT** is triggering our users to churn. This type of analyses is capable of showing business where is the bottlenecks of the system.\n",
        "\n",
        "Models from this notebook should be implement together by following next steps:\n",
        "- step 1: Implement account and interaction data to train long & short term models that will revield users that have higher probability of churning from services.\n",
        "- step 2: Implement uplift model as shown in this notebook, to estimate what is best strategy for retaining individual user, and use output of the model to optimise company retain budget.\n",
        "- step 3: Utilise user generated data to extract churn triggers in a system. \n",
        "\n",
        "The notebook is supposed to be run by Google Collaboratory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7aKhnlkD6DV"
      },
      "source": [
        "## Initialization and authentification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qY0AUBFrD_Kz"
      },
      "source": [
        "In this section, we perfom basic initialization and set some variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1QKwFAjSu6MV",
        "outputId": "c2bef787-ecac-44c6-f7c5-348e424c1e3a"
      },
      "outputs": [],
      "source": [
        "#\n",
        "# Reinstall google-cloud-aiplatform\n",
        "#\n",
        "\n",
        "!pip3 uninstall -y google-cloud-aiplatform\n",
        "!pip3 install google-cloud-aiplatform\n",
        " \n",
        "import IPython\n",
        " \n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HYRWdTe4W2t3",
        "outputId": "c2fe3290-d6e6-4c15-e3fc-afbf41942a84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting waterfallcharts\n",
            "  Downloading waterfallcharts-3.8.tar.gz (3.9 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hBuilding wheels for collected packages: waterfallcharts\n",
            "  Building wheel for waterfallcharts (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for waterfallcharts: filename=waterfallcharts-3.8-py3-none-any.whl size=3413 sha256=b9d213ed1624ee6bc595e37215934aa29e703af1d5e1540f4d380d153737dcda\n",
            "  Stored in directory: /Users/marko/Library/Caches/pip/wheels/41/09/98/4a4c399b27ecf43c049f7dde966823fcc688edde795d1b0d22\n",
            "Successfully built waterfallcharts\n",
            "Installing collected packages: waterfallcharts\n",
            "Successfully installed waterfallcharts-3.8\n",
            "Requirement already satisfied: pyyaml in /opt/anaconda3/envs/churn-prediction-vertex-ai/lib/python3.7/site-packages (6.0)\n",
            "/opt/anaconda3/envs/churn-prediction-vertex-ai/bin/python: No module named spacy\n"
          ]
        }
      ],
      "source": [
        "!pip install waterfallcharts\n",
        "!pip install pyyaml\n",
        "!python -m spacy download en_core_web_md "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GTbM_Lf3vFdH"
      },
      "outputs": [],
      "source": [
        "#\n",
        "# Authentication\n",
        "#\n",
        "\n",
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#\n",
        "# Pull starting kit folder from github repo\n",
        "#\n",
        "\n",
        "TOKEN = '[CHANGE TO YOUR GITHUB TOKEN]'\n",
        "!mkdir rnd-gcp-starter-kits && \\\n",
        "  cd rnd-gcp-starter-kits && \\\n",
        "  git init && \\\n",
        "  git remote add -f origin  https://{TOKEN}@github.com/griddynamics/rnd-gcp-starter-kits/ && \\\n",
        "  git config core.sparseCheckout true && \\\n",
        "  echo churn-prevention-vertex-ai >> .git/info/sparse-checkout && \\\n",
        "  git pull origin master"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#\n",
        "# Load configuration file, if needed than \n",
        "#\n",
        "\n",
        "import yaml\n",
        "\n",
        "config = yaml.safe_load(open(f'rnd-gcp-starter-kits/churn-prevention-vertex-ai/config.yml'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzYj6COWvR5O",
        "outputId": "c3925a5e-61ee-4c9b-9704-68d47f7cb153"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022_11_01_102625\n"
          ]
        }
      ],
      "source": [
        "#\n",
        "# Set some global variables with your project details and other information:\n",
        "#\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "TIMESTAMP = datetime.now().strftime(\"%Y_%m_%d_%H%M%S\")\n",
        "\n",
        "REGION = config['gcp']['region']\n",
        "PROJECT_ID = config['gcp']['project_id']\n",
        "BUCKET_NAME = config['gcp']['bucket_name']\n",
        "\n",
        "print(TIMESTAMP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "#\n",
        "# Create project, bucket and upload files \n",
        "#\n",
        "\n",
        "# # check if project and bucket exist if no create it\n",
        "# find_project = !gcloud projects list --filter $PROJECT_ID\n",
        "# if 'Listed 0 items.' in find_project:\n",
        "#    !gcloud projects create {PROJECT_ID}\n",
        "\n",
        "# # set project\n",
        "# !gcloud config set project {PROJECT_ID}\n",
        "\n",
        "# # check if bucket exist\n",
        "# from google.cloud import storage\n",
        "#\n",
        "# client = storage.Client()\n",
        "# try:\n",
        "#     bucket = client.get_bucket(BUCKET_NAME)\n",
        "# except:\n",
        "#     !gsutil mb gs://{BUCKET_NAME}\n",
        "\n",
        "# # transfer files to bucket\n",
        "# for key, file in config['data']['input_data'].items():\n",
        "#     if 'file_name' in key:\n",
        "#         !gsutil cp data/{file} gs://{BUCKET_NAME}/\n",
        "#     elif 'file_path' in key:\n",
        "#         !gsutil cp {file} gs://{BUCKET_NAME}/\n",
        "#     print(\"Copy {file} to {BUCKET_NAME}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yvYeiFM1vXKH"
      },
      "outputs": [],
      "source": [
        "#\n",
        "# Initialize AI platform:\n",
        "#\n",
        "\n",
        "from google.cloud import aiplatform\n",
        "import os\n",
        "\n",
        "aiplatform.init(project=PROJECT_ID, staging_bucket=BUCKET_NAME)\n",
        "os.environ[\"GCLOUD_PROJECT\"] = PROJECT_ID "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dl5Il6tBiArq"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rd-5fedr2-e7"
      },
      "source": [
        "## Useful functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "6FcDwDtmCFIZ",
        "outputId": "f766154a-79ac-47bc-f1bf-2934f0ad7ba1"
      },
      "outputs": [],
      "source": [
        "#\n",
        "# Function used to cosmetically format plots\n",
        "#\n",
        "\n",
        "def format_plots():\n",
        "\n",
        "    import seaborn as sns\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    sns.set(\n",
        "        font='serif',\n",
        "        rc={\n",
        "          'axes.axisbelow': False,\n",
        "          'axes.edgecolor': 'lightgrey',\n",
        "          'axes.facecolor': 'None',\n",
        "          'axes.grid': False,\n",
        "          'axes.labelcolor': 'dimgrey',\n",
        "          'axes.spines.right': False,\n",
        "          'axes.spines.top': False,\n",
        "          'figure.facecolor': 'white',\n",
        "          'lines.solid_capstyle': 'round',\n",
        "          'patch.edgecolor': 'w',\n",
        "          'patch.force_edgecolor': True,\n",
        "          'text.color': 'black',\n",
        "          'xtick.bottom': False,\n",
        "          'xtick.color': 'dimgrey',\n",
        "          'xtick.direction': 'out',\n",
        "          'xtick.top': False,\n",
        "          'ytick.color': 'dimgrey',\n",
        "          'ytick.direction': 'out',\n",
        "          'ytick.left': False,\n",
        "          'ytick.right': False}\n",
        "    )\n",
        "    sns.set_context(\n",
        "        \"notebook\", \n",
        "        rc={\n",
        "          'font.size':14,\n",
        "          'axes.titlesize':14,\n",
        "          'axes.labelsize':14}\n",
        "    )\n",
        "\n",
        "    plt.rcParams[\"figure.figsize\"] = [10, 8]\n",
        "    plt.rcParams[\"figure.autolayout\"] = True\n",
        "    plt.rcParams['axes.prop_cycle'] = plt.cycler(color=[\n",
        "        '#2CBDFE', '#47DBCD', '#F3A0F2', '#9D2EC5', '#661D98', '#F5B14C'\n",
        "    ])\n",
        "    sns.despine(left=True, bottom=True)\n",
        "\n",
        "format_plots()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WHL-TpDXlTRD"
      },
      "outputs": [],
      "source": [
        "#\n",
        "# Read files from bucket\n",
        "#\n",
        "\n",
        "def bucket_to_bytes(file_name, bucket_name=BUCKET_NAME):\n",
        "\n",
        "    from google.cloud import storage\n",
        "    from io import BytesIO\n",
        "\n",
        "    if f\"gs://{bucket_name}/\" in file_name:\n",
        "        file_name = file_name.replace(f\"gs://{bucket_name}/\", \"\")\n",
        "\n",
        "    # Get the blob\n",
        "    storage_client = storage.Client()\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "    blob = bucket.blob(file_name)\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    contents = blob.download_as_bytes()\n",
        "    data = BytesIO(contents)\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v5xld-GblWgz"
      },
      "outputs": [],
      "source": [
        "#\n",
        "# Read csv file as a Pandas DataFrame from the bucket\n",
        "#\n",
        "\n",
        "def read_csv_from_bucket(gcs_path, bucket_name=BUCKET_NAME):\n",
        "\n",
        "    import pandas as pd\n",
        "    csv_data = bucket_to_bytes(gcs_path, bucket_name)\n",
        "    return pd.read_csv(csv_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pmA-ixrH3Dh4"
      },
      "outputs": [],
      "source": [
        "#\n",
        "# Write csv file to the bucket\n",
        "#\n",
        "\n",
        "def write_csv_to_bucket(csv_object, gcs_path, bucket_name=BUCKET_NAME):\n",
        "\n",
        "    from google.cloud import storage\n",
        "\n",
        "    storage_client = storage.Client()\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "    bucket.blob(gcs_path).upload_from_string(csv_object, 'text/csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TNCY0sHRLq_M"
      },
      "outputs": [],
      "source": [
        "#\n",
        "# Evaluation of Vertex AI classification model\n",
        "#\n",
        "\n",
        "def model_evaluation(model, cm_cols=['retain', 'churn'], highlight_feature=None):\n",
        "\n",
        "    import pandas as pd\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    for model_evaluation in model.list_model_evaluations():\n",
        "\n",
        "        info_dict = model_evaluation.to_dict()\n",
        "        info_metrics = info_dict['metrics']\n",
        "        info_features = info_dict['modelExplanation']\n",
        "        confusion_matrix = info_metrics['confusionMatrix']\n",
        "        au_prc = info_metrics['auPrc']\n",
        "        au_roc = info_metrics['auRoc']\n",
        "        log_loss = info_metrics['logLoss']\n",
        "\n",
        "    info_dict['cm_df'] = pd.DataFrame(confusion_matrix['rows'], \n",
        "        index=cm_cols, columns=cm_cols)\n",
        "    TP = confusion_matrix['rows'][1][1]\n",
        "    FP = confusion_matrix['rows'][0][1]\n",
        "    FN = confusion_matrix['rows'][1][0]\n",
        "    precision = round(TP / (FP + TP), 2)\n",
        "    recall = round(TP / (FN + TP), 2)\n",
        "\n",
        "    print(\"Model metrics:\\n\")\n",
        "    print(f\" - Area under Precision-Recall Curve: {au_prc}\")\n",
        "    print(f\" - Area under Receiver Operating Characteristic Curve: {au_roc}\")\n",
        "    print(f\" - Log-loss: {log_loss}\")\n",
        "    print(f\" - Precision: {precision}\")\n",
        "    print(f\" - Recall: {recall}\")\n",
        "\n",
        "    print(\"\\nConfusion matrix:\\n\")\n",
        "    print(info_dict['cm_df'])\n",
        "\n",
        "    feature_importance = info_features['meanAttributions'][0]['featureAttributions']\n",
        "    sorted_feature_importance = sorted(\n",
        "        feature_importance.items(), key=lambda x: x[1], reverse=False)\n",
        "    names = [feature for feature, _ in sorted_feature_importance]\n",
        "    values = [value for _, value in sorted_feature_importance]\n",
        "\n",
        "    plt.barh(range(len(feature_importance)), values, tick_label=names)\n",
        "    if highlight_feature in names:\n",
        "        plt.gca().get_yticklabels()[int(names.index(highlight_feature))].set_color(\"red\")\n",
        "    plt.title(\"Feature importance\")\n",
        "    plt.show()\n",
        "    \n",
        "    return info_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v-J63F8sTHuU"
      },
      "outputs": [],
      "source": [
        "#\n",
        "# Optimise threshold classification by using costs\n",
        "#\n",
        "\n",
        "def optimisation_by_cost_matrix(tresholds, f_p_cost, f_n_cost, p_c):\n",
        "\n",
        "    import pandas as pd\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    costs = []\n",
        "    for i in range(len(tresholds)):\n",
        "      try:\n",
        "        false_negative = int(tresholds[i]['falseNegativeCount'])\n",
        "      except KeyError:\n",
        "        false_negative = 0\n",
        "      \n",
        "      try:\n",
        "        false_positive = int(tresholds[i]['falsePositiveCount'])\n",
        "      except KeyError:\n",
        "        false_positive = 0\n",
        "\n",
        "      costs.append(f_p_cost*false_positive + f_n_cost*false_negative*p_c)\n",
        "\n",
        "    best_threshold = tresholds[int(costs.index(min(costs)))]['confidenceThreshold']\n",
        "    print(f\"Minimum costs of {min(costs)}$ are obtained for {best_threshold} \\\n",
        "    threshold.\")\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(7,4))\n",
        "    ax.plot(np.linspace(0,1,len(tresholds)),costs)\n",
        "    ax.scatter(best_threshold,min(costs), c='r', linewidths=25)\n",
        "    ax.set(xlabel='threshold', ylabel='cost [$]',\n",
        "          title='Cost VS threshold')\n",
        "    ax.grid()\n",
        "\n",
        "    plt.show()\n",
        "    return best_threshold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A6931NNPWQsQ"
      },
      "outputs": [],
      "source": [
        "#\n",
        "# Get predictions in batch mode\n",
        "#\n",
        "\n",
        "def get_df_from_batch_predict(\n",
        "    input_file,\n",
        "    output_folder,\n",
        "    model_name,\n",
        "    bucket_name,\n",
        "    model_object=None,\n",
        "    model_path=None,\n",
        "):\n",
        "    import pandas as pd\n",
        "    \n",
        "    if model_path is None:\n",
        "\n",
        "        if model_object is None:\n",
        "            raise AttributeError(\"If model_path is None, than model_object\\\n",
        "             need to be instance of model\")\n",
        "            \n",
        "        print(f\"Batch predictions with {model_name} model.\")\n",
        "        \n",
        "        batch_predict_job = model_object.batch_predict(\n",
        "          gcs_source=input_file,\n",
        "          instances_format=\"csv\",\n",
        "          gcs_destination_prefix=output_folder,\n",
        "          predictions_format=\"csv\",\n",
        "          job_display_name=f\"job-batch_predict-{model_name}-{TIMESTAMP}\",\n",
        "          sync=True\n",
        "        )\n",
        "        batch_predict_job.wait()\n",
        "        list_files = batch_predict_job.iter_outputs()\n",
        "        print(batch_predict_job.output_info.gcs_output_directory)\n",
        "\n",
        "    else:\n",
        "        \n",
        "        from google.cloud import storage\n",
        "        \n",
        "        storage_client = storage.Client()\n",
        "        bucket = storage_client.get_bucket(bucket_name)\n",
        "        list_files = list(bucket.list_blobs(prefix=model_path))\n",
        "\n",
        "    df_list = []\n",
        "    for row in list_files:\n",
        "        df_list.append(read_csv_from_bucket(\n",
        "            row.name, bucket_name=bucket_name))\n",
        "\n",
        "    return pd.concat(df_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZ4py4CrBsZd"
      },
      "outputs": [],
      "source": [
        "#\n",
        "# Plot time-series of prediction churn periods \n",
        "#\n",
        "\n",
        "def plot_uuid_ts_with_periods(\n",
        "    uuid, \n",
        "    df_predict_interaction, \n",
        "    df_lt_result, \n",
        "    thresholds,\n",
        "    map_int_to_date):\n",
        "    \n",
        "    import matplotlib.pyplot as plt\n",
        "    import pandas as pd\n",
        "    from dateutil.relativedelta import relativedelta\n",
        "  \n",
        "    print(f\"User id: {uuid}\")\n",
        "\n",
        "    fig_size = (10, 15)\n",
        "    fig, axs = plt.subplots(ncols=1, nrows=4, figsize=fig_size, \n",
        "                            gridspec_kw={'height_ratios': [1, 1, 1, 2]})\n",
        "    for i, col in enumerate(['internet_min', 'phone_min', 'number_customer_service_calls']):\n",
        "\n",
        "        df_tmp = df_predict_interaction.loc[(df_predict_interaction['uuid']==uuid)].iloc[-7:]\n",
        "\n",
        "        df_tmp.index = pd.to_datetime(df_tmp['timestamp'])\n",
        "        df_tmp_pred = df_lt_result.loc[(df_lt_result['uuid']==uuid)]\n",
        "        df_tmp_pred.index = pd.to_datetime(df_tmp_pred['timestamp'])\n",
        "\n",
        "        df_tmp_pred = df_lt_result.loc[(df_lt_result['timestamp']=='2021-10-01')&(df_lt_result['uuid']==uuid)\n",
        "          ].sort_values(by='time_horizon')[['time_horizon', 'churn', 'churn_1_scores', col]]\n",
        "        df_tmp_pred = df_tmp_pred.replace({\"time_horizon\": map_int_to_date})\n",
        "        df_tmp_pred.index = pd.to_datetime(df_tmp_pred['time_horizon'])\n",
        "\n",
        "        df_tmp.loc[df_tmp['uuid']==uuid, [col]].plot(ax=axs[i])\n",
        "        x_true_neg = df_tmp_pred.loc[(df_tmp_pred['churn']==0)].index\n",
        "        x_true_pos = df_tmp_pred.loc[(df_tmp_pred['churn']==1)].index\n",
        "\n",
        "\n",
        "        # x_pred_neg = df_tmp_pred.loc[(df_tmp_pred['churn_1_scores']<=.5)].index\n",
        "        # x_pred_pos = df_tmp_pred.loc[(df_tmp_pred['churn_1_scores']>=.5)].index\n",
        "\n",
        "        # max_int = max(df_tmp[col].max(), 1)\n",
        "\n",
        "        if len(x_true_pos)>0:\n",
        "            x_true_pos = sorted([min(list(x_true_pos)) - relativedelta(months=1)] + list(x_true_pos))\n",
        "            x_true_pos = [list(x_true_pos)[0] + relativedelta(months=1)] + list(x_true_pos)\n",
        "            y = df_tmp.loc[df_tmp.index.isin(x_true_pos), col].values\n",
        "            if len(y)< len(x_true_pos):\n",
        "                x_true_pos = x_true_pos[1:]\n",
        "            axs[i].fill_between(x_true_pos, y, color='red', alpha=.2, label='true churn period')\n",
        "\n",
        "        if len(x_true_neg)>0:\n",
        "            x_true_neg = sorted([min(list(x_true_neg)) - relativedelta(months=1)] + list(x_true_neg))\n",
        "            \n",
        "            y = df_tmp.loc[df_tmp.index.isin(x_true_neg), col].values\n",
        "            if len(y)< len(x_true_neg):\n",
        "                x_true_neg = x_true_neg[1:]\n",
        "            axs[i].fill_between(x_true_neg, y, color='blue', alpha=.2, label='true retention period')\n",
        "        axs[i].legend(loc='upper left')\n",
        "        axs[i].set_title(f\"True values for \\nperiods of {col} ts feature\")\n",
        "        axs[i].set_xlabel('Months')\n",
        "        axs[i].set_ylabel('min' if col != 'no_service_calls' else 'count')\n",
        "    \n",
        "    if len(df_tmp_pred.index) >= len(map_int_to_date):\n",
        "        axs[3].bar(\n",
        "            df_tmp_pred.index, \n",
        "            df_tmp_pred['churn_1_scores'].values,\n",
        "            width = 10,\n",
        "            label = 'churn probability',\n",
        "            )\n",
        "        axs[3].step(\n",
        "            df_tmp_pred.index.values, \n",
        "            list(thresholds.values()), \n",
        "            'k',\n",
        "            where=\"mid\",\n",
        "            linestyle='--',\n",
        "            linewidth=2,\n",
        "            label = 'threshold'\n",
        "            )\n",
        "\n",
        "    axs[3].set_title(f\"Risk of churning\")\n",
        "    axs[3].legend(loc='center left')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhPzsy_TEZhB"
      },
      "source": [
        "## Load data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmyuD5xUyS44"
      },
      "source": [
        "In many cases, user data is very sensitive and because of it's complience it cannot be used as it is for the purpose of this notbook. Moreover, if we took into consideration that main purpose of this starting kit is to show best practices for implementing churn prediction methodology with Vertex AI, it is not crucial to have real data for demonstration purposes and therefore we decided to proceed with syntheticly generated data. There are three types of data generated for this exercise, and that is:\n",
        "\n",
        " - Account data - this is data that is usually stored in data warehouses and it consist from user social-demografic, duration of contract, type of payment and etc.\n",
        " - Interaction data - this is data that is usually stored in transactional database and it consisted from user generated events, such as payment transactions, usage of system during time, logs and etc. \n",
        " - User generated data - this data is usually stored in data lake and it consisted of unstructural data, such as email, call transcription, chats and etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCm-J43tnnnO"
      },
      "source": [
        "### Account data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "isNF8BBhLELR",
        "outputId": "852e7465-8526-4682-baf8-e2cd0e8a8ef0"
      },
      "outputs": [],
      "source": [
        "#\n",
        "# Read account data\n",
        "#\n",
        "\n",
        "CSV_TRAIN_ACCOUNT = os.path.join(\"gs://\", BUCKET_NAME, config['data']['input_data']['file_name_short_term_train'])\n",
        "CSV_PREDICT_ACCOUNT = os.path.join(\"gs://\", BUCKET_NAME, config['data']['input_data']['file_name_short_term_predict'])\n",
        "DATASET_ACCOUNT = aiplatform.TabularDataset.create(\n",
        "    display_name=\"account_data_train\", gcs_source=[CSV_TRAIN_ACCOUNT])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "id": "7qo4Q9I9aVK9",
        "outputId": "08861baa-a140-402e-d48d-a24e3ec474d7"
      },
      "outputs": [],
      "source": [
        "df_train_account = read_csv_from_bucket(CSV_TRAIN_ACCOUNT)\n",
        "df_predict_account = read_csv_from_bucket(CSV_PREDICT_ACCOUNT)\n",
        "df_train_account.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAJs3ekxZIU9"
      },
      "source": [
        "*   `uuid` - unique user id\n",
        "*   `gender` - Male/Female\n",
        "*   `tenure` - number of consecutive months that user is subscribed to a service\n",
        "*   `phone_services` - 0 or 1 that shows if user use phone services\n",
        "*   `internet_services` - 0 or 1 that shows if user use internet services\n",
        "*   `contract_duration` - short/long type of contract\n",
        "*   `payment_method` - email/mail/automatic type of payment\n",
        "*   `number_customer_service_calls` - total number of calls to the customer services\n",
        "*   `phone_min` - total duration of phone calls in last month\n",
        "*   `internet_min` - total duration of internet usage in last month\n",
        "*   `phone_monthly_charges` - phone call bill for last month\n",
        "*   `internet_monthly_charges` - internet bill for last month\n",
        "*   `avg_monthly_bill` - average bill during user lifetime\n",
        "*   `trigger_price` - 0 or 1 that shows if price trigger user to churn\n",
        "*   `trigger_quality` - 0 or 1 that shows if quality of service trigger user to churn\n",
        "*   `trigger_external` - 0 or 1 that shows if external factor trigger user to churn\n",
        "*   `churn` - 0 or 1 that shows if user churn or not\n",
        "*   `treatment` - none/discount/upgrade package/free device treatment implemented in order to retain user\n",
        "*   `churn_after_treatment` - 0 or 1 that shows if user retain after treatment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2eY_5-6cId7"
      },
      "source": [
        "Usually, churn datasets are imbalanced, that means that there are more retaine users than churned ones. Consequently, this is the case in current dataset as well, with 26% of churned users. Moreover, this will be one of the point that we need to address during the training phase."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 577
        },
        "id": "1IepCHjLs-lr",
        "outputId": "5190bcaf-799d-49b5-af2b-c1819d248680"
      },
      "outputs": [],
      "source": [
        "ax = df_train_account.replace(\n",
        "        {'churn': {0: \"Retain users\", 1: \"Churn users\"}}\n",
        "    ).groupby('churn')['churn'].count().plot(\n",
        "        kind=\"pie\", autopct='%1.1f%%', shadow=True, explode=[0.05, 0.05], \n",
        "        legend=True, title='Ratio between churn and retain users', \n",
        "        ylabel='', labeldistance=None\n",
        "    )\n",
        "    \n",
        "ax.legend(bbox_to_anchor=(1, 1), loc='upper left', prop={'size': 15})\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DX9b6sOkcEw_"
      },
      "source": [
        "### Interaction data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4pSN2T12gbjH",
        "outputId": "9546c0bf-4a78-479a-9bce-bb3b7b12bf11"
      },
      "outputs": [],
      "source": [
        "CSV_TRAIN_INTERACTION = os.path.join(\"gs://\", BUCKET_NAME, config['data']['input_data']['file_name_long_term_v1_train'])\n",
        "CSV_PREDICT_INTERACTION = os.path.join(\"gs://\", BUCKET_NAME, config['data']['input_data']['file_name_long_term_v1_predict'])\n",
        "DATASET_INTERACTION = aiplatform.TimeSeriesDataset.create(\n",
        "  display_name=\"timeseries_data_train\", gcs_source=[CSV_TRAIN_INTERACTION])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "1Zb7xPkjij4R",
        "outputId": "88891a25-b0c7-447b-c8f1-290c81674f1f"
      },
      "outputs": [],
      "source": [
        "df_train_interaction = read_csv_from_bucket(CSV_TRAIN_INTERACTION)\n",
        "df_predict_interaction = read_csv_from_bucket(CSV_PREDICT_INTERACTION)\n",
        "df_train_interaction.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAJW3_pfiyb9"
      },
      "source": [
        "*   `uuid` - unique user id\n",
        "*   `timestamp` - time when record is generated\n",
        "*   `phone_min` - total duration of phone calls during time window of one month\n",
        "*   `internet_min` - total duration of internet usage during time window of one month\n",
        "*   `number_customer_service_calls` - total number of calls to the customer services during time window of one month\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTCH-U9xLNIR"
      },
      "source": [
        "### Combine data-set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19_DsA4jXuQ-"
      },
      "source": [
        "In this part we are loading data that obtained by merging accoutn and interaction data, combined in a way so that could be used for training Tabular models. That is, we introduced several new features like internet_min_lag1, internet_min_lag2, internet_min_lag3 and etc. where we add data from previous period, while we add a feature time_horizont for determine for which period in a future we want to make a prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23foVrA0LMb9",
        "outputId": "959df591-32f9-4dfd-eb6f-8ce7b233fc7b"
      },
      "outputs": [],
      "source": [
        "CSV_TRAIN_ACC_INTER = os.path.join(\"gs://\", BUCKET_NAME, config['data']['input_data']['file_name_combined_data_train'])\n",
        "CSV_PREDICT_ACC_INTER = os.path.join(\"gs://\", BUCKET_NAME, config['data']['input_data']['file_name_combined_data_predict'])\n",
        "DATASET_ACC_INTER = aiplatform.TabularDataset.create(\n",
        "    display_name=\"account_data_train\", gcs_source=[CSV_TRAIN_ACC_INTER])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 560
        },
        "id": "LCM9zaHxLhzw",
        "outputId": "7012ba76-ca68-4df3-c0ef-68e787aabfe9"
      },
      "outputs": [],
      "source": [
        "df_train_acc_inter = read_csv_from_bucket(CSV_TRAIN_ACC_INTER)\n",
        "df_predict_acc_inter = read_csv_from_bucket(CSV_PREDICT_ACC_INTER)\n",
        "df_train_acc_inter.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Y_wLQAfQvxp",
        "outputId": "56d54759-9e5d-432b-d81f-cb6508e27c96"
      },
      "outputs": [],
      "source": [
        "CSV_TRAIN_ACC_INTER_V2 = os.path.join(\"gs://\", BUCKET_NAME, config['data']['input_data']['file_name_long_term_v2_train'])\n",
        "CSV_PREDICT_ACC_INTER_V2 = os.path.join(\"gs://\", BUCKET_NAME, config['data']['input_data']['file_name_long_term_v2_predict'])\n",
        "DATASET_ACC_INTER_V2 = aiplatform.TimeSeriesDataset.create(\n",
        "  display_name=\"timeseries_data_train_forecast\", gcs_source=[CSV_TRAIN_ACC_INTER_V2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 531
        },
        "id": "vYZYP_ZhYnps",
        "outputId": "deca3360-f254-45d4-ae6a-d9b45e0b9dbd"
      },
      "outputs": [],
      "source": [
        "df_train_acc_inter_v2 = read_csv_from_bucket(CSV_TRAIN_ACC_INTER_V2)\n",
        "df_predict_acc_inter_v2 = read_csv_from_bucket(CSV_PREDICT_ACC_INTER_V2)\n",
        "df_train_acc_inter_v2.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8_qVG5d2jgs"
      },
      "source": [
        "## Short-term churn prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFx1rOMDI5zV"
      },
      "source": [
        "### Train model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpJ-ZPbw08sy"
      },
      "source": [
        "As it was mantion previously, dataset used for training this model is imbalanced. That means, that more users retain in the system than the ones that churnes. Therefor, we are going to use 'maximize-au-prc' objective function. By looking at Vertex AI documentation, we may see that this function is used in a case when we want to optimize for less common class (in this case churned users), and it stands for area under the precision-recall curve."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qSSyxQILSjc0"
      },
      "outputs": [],
      "source": [
        "exclude = ['uuid']\n",
        "target_column = 'churn'\n",
        "multi_target_columns = ['trigger_price', 'trigger_quality', 'trigger_external']\n",
        "treatments = ['churn_after_treatment', 'treatment']\n",
        "cols = [col for col in df_train_account.columns \n",
        "        if col not in exclude+[target_column]+multi_target_columns+treatments]\n",
        "categorical_cols = df_train_account.loc[:, cols].select_dtypes(exclude=\"number\").columns\n",
        "numerical_cols = df_train_account.loc[:, cols].select_dtypes(include=\"number\").columns\n",
        "\n",
        "COLUMN_SPEC = {}\n",
        "for col_i in numerical_cols:\n",
        "  COLUMN_SPEC[col_i] = \"numeric\"\n",
        "for col_i in categorical_cols:\n",
        "  COLUMN_SPEC[col_i] = \"categorical\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-zV9CwmN6MF"
      },
      "outputs": [],
      "source": [
        "model_st_id = config['artifacts']['model_id']['short_term_churn_model']\n",
        "model_st_display_name = \"aggregate-churn-prediction-model\"\n",
        "\n",
        "if model_st_id is None:\n",
        "  job_tabular = aiplatform.AutoMLTabularTrainingJob(\n",
        "      display_name=f'job_train_model-{model_st_display_name}-{TIMESTAMP}',\n",
        "      optimization_prediction_type='classification',\n",
        "      optimization_objective='maximize-au-prc',\n",
        "      column_specs=COLUMN_SPEC\n",
        "  )\n",
        "\n",
        "  model_st = job_tabular.run(\n",
        "      dataset = DATASET_ACCOUNT,\n",
        "      target_column = target_column,\n",
        "      training_fraction_split = 0.7,\n",
        "      validation_fraction_split = 0.15,\n",
        "      test_fraction_split = 0.15,\n",
        "      budget_milli_node_hours=10, #1000\n",
        "      model_display_name=model_st_display_name,\n",
        "  )\n",
        "else:\n",
        "  model_st = aiplatform.Model(model_st_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzSB9lw-I8jc"
      },
      "source": [
        "### Analyse model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 803
        },
        "id": "fwsk2Y47KzLG",
        "outputId": "5ac6728a-ccbd-4105-bdfa-15bd8805e9df"
      },
      "outputs": [],
      "source": [
        "info_eval = model_evaluation(model_st)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8IeoaGKx1Q3"
      },
      "source": [
        "Feature importance reveals which features contributes the most in classifing churners. Therefore, we are going to utilise it to take a closer inspection of distributions of the most promenent ones (average monthly bills, charges for internet and internet minutes). In addition, comparing to other features, this graph shows that gender, contract duration, phone and internet services plays little role in defining churners."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 810
        },
        "id": "Wo9OIEgdAyrf",
        "outputId": "571829d1-e1cf-400a-a080-1d4ff3a3821d"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(ncols=1, nrows=3, figsize=(10, 10))\n",
        "axs[0].set_title(\"Compare features of churns and retain users\")\n",
        "axs[0].hist(\n",
        "    [df_train_account.loc[df_train_account['churn']==1, 'internet_monthly_charges'].tolist(),\n",
        "     df_train_account.loc[df_train_account['churn']==0, 'internet_monthly_charges'].tolist()], \n",
        "     label=['churn','retain'],\n",
        "     density=True)\n",
        "axs[0].set_xlabel(\"internet monthly charges [$]\")\n",
        "axs[0].legend(loc='upper right')\n",
        "axs[1].hist(\n",
        "    [df_train_account.loc[df_train_account['churn']==1, 'internet_min'].tolist(),\n",
        "     df_train_account.loc[df_train_account['churn']==0, 'internet_min'].tolist()], \n",
        "     label=['churn','retain'],\n",
        "     density=True)\n",
        "axs[1].set_xlabel(\"internet usage [min]\")\n",
        "axs[2].hist(\n",
        "    [df_train_account.loc[df_train_account['churn']==1, 'tenure'].tolist(),\n",
        "     df_train_account.loc[df_train_account['churn']==0, 'tenure'].tolist()], \n",
        "     label=['churn','retain'],\n",
        "     density=True)\n",
        "axs[2].set_xlabel(\"user tenure [months]\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7fu6tgfz_pB"
      },
      "source": [
        "This type of analyses is out of the scope of this notebook, but it could be implemented further on in defining churn metrics. This type of metrics is valuable in buisness as a measure of showing how users that retain in a system use their product versus the ones that churned. Moreover, it could be used to push churners through customer support or some treatment which will convert them back to the services."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIhVrLVSK0UQ"
      },
      "source": [
        "### Optimisation by cost matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfylBQ6h2wz9"
      },
      "source": [
        "In general, in order to have better precision we need to sacrifice recall, and vice versa. Moreover, different businesses has different sensitivity when it comes to selecting recall vs precision. So, defining balance between those two metrics represents an important decision, because it will revield what threshold we should use and how it will effect our costs. One of the best practices in this case is to use cost matrix. \n",
        "\n",
        "In order to illustrate this, we add artificially costs as follow:\n",
        "- Recall\n",
        " - costs of replacing false pnegative users with new ones cost on average 40 dollars, \n",
        " - probability of keeping false negative if we would clasiffy them as churners is 0.3\n",
        " - Therefore, expected costs of not predicting churned users per case is 12 dollars. \n",
        "- Precision\n",
        " - Wrongly treated retain users as churned ones cost company 10 dollars per case, by promoting the system through one time down sell promotions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "fcKjmoviK-ar",
        "outputId": "9468d17f-dcc8-489d-da41-54d678071248"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# illustrative values\n",
        "down_sell_cost = 10\n",
        "acquisition_cost = 40\n",
        "prob_of_converting = 0.3\n",
        "\n",
        "\n",
        "expected_return_TN = df_train_account.loc[df_train_account.churn==0].avg_monthly_bill.mean()\n",
        "expected_return_TP = prob_of_converting * (\n",
        "    df_train_account.loc[df_train_account.churn==1].avg_monthly_bill.mean() - \n",
        "    down_sell_cost)\n",
        "expected_cost_FP = -1* down_sell_cost\n",
        "expected_cost_FN = -1* (acquisition_cost - down_sell_cost) * prob_of_converting\n",
        "cost_matrix = pd.DataFrame(\n",
        "    np.array([\n",
        "                        [expected_return_TN, expected_cost_FP], \n",
        "                        [expected_cost_FN, expected_return_TP]\n",
        "]).astype(int), index=['retain', 'churn'], columns=['retain', 'churn'])\n",
        "cost_matrix.style.apply(\n",
        "    lambda x: ['background-color: red' if v<=0 else \"\" for v in x]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "id": "T3AbxuRbS0IK",
        "outputId": "ec11ab5e-a63f-4dd7-e38f-c2133dd4ef03"
      },
      "outputs": [],
      "source": [
        "best_threshold = optimisation_by_cost_matrix(\n",
        "    info_eval['metrics']['confidenceMetrics'], \n",
        "    down_sell_cost, \n",
        "    acquisition_cost, \n",
        "    prob_of_converting)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYXiFsQEMFsB"
      },
      "source": [
        "## Long-term churn prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1EazY3Eml0PO"
      },
      "source": [
        "## Version 1: AutoMLTabular model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1V_THCauQe61"
      },
      "source": [
        "This section of the notebook address the problem of identifying churners before it is too late. Therefore, we will show the model that is capable of providing estimations several months before users unsubscribe from services. For this purpose, we implemented synthetically generated time series data, that is used for training time aware model with autoML Tabular (Vertex AI) service.\n",
        "\n",
        "Furthermore, we show that short-term models are better balanced and that they are more sensitive to churn behaviour. However, major downside of short-term models is that after it make predictions, buisness doesn't have so much time to act and in many cases users already made definite decision of churning from product. However, this is where long term predictions shine and it gave us ability to act on a time by predicting probability of churning for mid future period (6 months in this case). \n",
        "\n",
        "In general, we would like to keep both models, short-term models because they are more accurate and it reveals actual user intentions, while long-term model is better with identification of possible future churners."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7nSWj4mMUw7"
      },
      "source": [
        "### Train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4UGQTmW7MFNV"
      },
      "outputs": [],
      "source": [
        "exclude = ['uuid', 'timestamp']\n",
        "target_column = 'churn'\n",
        "multi_target_columns = ['trigger_price', 'trigger_quality', 'trigger_external']\n",
        "treatments = ['churn_after_treatment', 'treatment']\n",
        "cols = [col for col in df_train_acc_inter.columns \n",
        "        if col not in exclude+[target_column]+multi_target_columns+treatments]\n",
        "categorical_cols = df_train_acc_inter.loc[:, cols].select_dtypes(exclude=\"number\").columns\n",
        "numerical_cols = df_train_acc_inter.loc[:, cols].select_dtypes(include=\"number\").columns\n",
        "\n",
        "COLUMN_SPEC = {}\n",
        "for col_i in numerical_cols:\n",
        "  COLUMN_SPEC[col_i] = \"numeric\"\n",
        "for col_i in categorical_cols:\n",
        "  COLUMN_SPEC[col_i] = \"categorical\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RzOxI7WNMqrR"
      },
      "outputs": [],
      "source": [
        "model_lt_id = config['artifacts']['model_id']['long_term_churn_model_v1']\n",
        "model_lt_display_name = \"event-churn-prediction-model\"\n",
        "\n",
        "if model_lt_id is None:\n",
        "  job_tabular = aiplatform.AutoMLTabularTrainingJob(\n",
        "      display_name=f'job_train_model-{model_lt_display_name}-{TIMESTAMP}',\n",
        "      optimization_prediction_type='classification',\n",
        "      optimization_objective='maximize-au-prc',\n",
        "      column_specs=COLUMN_SPEC\n",
        "  )\n",
        "\n",
        "  model_lt = job_tabular.run(\n",
        "      dataset = DATASET_ACC_INTER,\n",
        "      target_column = target_column,\n",
        "      training_fraction_split = 0.7,\n",
        "      validation_fraction_split = 0.15,\n",
        "      test_fraction_split = 0.15,\n",
        "      budget_milli_node_hours=10,\n",
        "      model_display_name=model_lt_display_name,\n",
        "  )\n",
        "else:\n",
        "  model_lt = aiplatform.Model(model_lt_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhjoYRq_NM1q"
      },
      "source": [
        "### Analyse model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 803
        },
        "id": "fBSiuy7ANQt7",
        "outputId": "a2fcf8b6-1494-4d99-cf66-eeb7b714b243"
      },
      "outputs": [],
      "source": [
        "info_eval = model_evaluation(model_lt, highlight_feature='time_horizon')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ZBCvz8pYPHR"
      },
      "outputs": [],
      "source": [
        "DATA_LT_FOLDER = config['artifacts']['model_path']['long_term_churn_model_v1_folder']\n",
        "model_lt_results = config['data']['output_data']['file_path_long_term_churn_model_v1']\n",
        "gsc_output_folder = os.path.join(\"gs://\", BUCKET_NAME, DATA_LT_FOLDER, f\"predictions-st-{TIMESTAMP}\")\n",
        "\n",
        "df_lt_result = get_df_from_batch_predict(\n",
        "    CSV_PREDICT_ACC_INTER, \n",
        "    gsc_output_folder, \n",
        "    model_lt_display_name,\n",
        "    BUCKET_NAME,\n",
        "    model_lt,\n",
        "    model_lt_results)\n",
        "\n",
        "df_lt_result['predicted_churn'] = 0\n",
        "df_lt_result.loc[\n",
        "    df_lt_result['churn_0_scores']<=.5, 'predicted_churn'] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gm8-zq4mZ8f8"
      },
      "outputs": [],
      "source": [
        "threshodls_by_time_horizont = {}\n",
        "tresholds = np.linspace(0,1, 101)\n",
        "for time_horizont in range(1,7):\n",
        "    df_tmp = df_lt_result.loc[df_lt_result['time_horizon']==time_horizont, ['churn_1_scores', 'churn']]\n",
        "    costs = []\n",
        "    for th in tresholds:\n",
        "        FP = df_tmp.loc[(df_tmp['churn']==0)& (df_tmp['churn_1_scores']>=th)].shape[0]\n",
        "        FN = df_tmp.loc[(df_tmp['churn']==1)& (df_tmp['churn_1_scores']<th)].shape[0]\n",
        "        costs.append(\n",
        "            down_sell_cost*FP + \n",
        "            acquisition_cost*FN*prob_of_converting\n",
        "        )\n",
        "    threshodls_by_time_horizont[time_horizont] = tresholds[int(costs.index(min(costs)))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZcVTbz7okoH_"
      },
      "outputs": [],
      "source": [
        "map_dates = {\n",
        "    1: '2021-10-01',\n",
        "    2: '2021-11-01',\n",
        "    3: '2021-12-01',\n",
        "    4: '2022-01-01',\n",
        "    5: '2022-02-01',\n",
        "    6: '2022-03-01'\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "VJs5-KS4XG_x",
        "outputId": "4a92d860-1cd1-41b7-f0d2-7b564dab7820"
      },
      "outputs": [],
      "source": [
        "plot_uuid_ts_with_periods(\n",
        "        '8a43ef1e-0756-11ed-a65f-0242ac1c0002', \n",
        "        df_predict_interaction, \n",
        "        df_lt_result, \n",
        "        threshodls_by_time_horizont,\n",
        "        map_dates\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pqWxD6BHXJks",
        "outputId": "2e78f047-af60-4663-aaca-28e00f939c14"
      },
      "outputs": [],
      "source": [
        "plot_uuid_ts_with_periods(\n",
        "        '8a4721ca-0756-11ed-a65f-0242ac1c0002', \n",
        "        df_predict_interaction, \n",
        "        df_lt_result, \n",
        "        threshodls_by_time_horizont,\n",
        "        map_dates\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Wpiv0crSxW2"
      },
      "source": [
        "## Version 2: AutoMLForecasting model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwdS2z8ZmJoi"
      },
      "source": [
        "In implementing sequence model, we faced with two major challenges. \n",
        "\n",
        "1.   One of them were how to transform classification to regression problem. Basically, churn prediction labels in data represent revealed probability that user will churn, therefore we have only two labels 0 that user retain and 1 that user churn. By interpreting a label as probability of churnes, the problem is reframed as a regression one. Therefore, we may use inverse sigmoid function to transform labels in a more easilly \"digested\" values for time serias model. \n",
        "\n",
        "> $$ x_{regression} =  \\dfrac{1}{1+e^{-x}} $$\n",
        "\n",
        "2.   Second, dataset is even more unbalanced, that is churnes represent rare events in time serias. We approach this problem, by smoothing values near churn event with 0.5 probability values that represent uninformative information, and underline justification is that we are uncertanty whether user decided to churn during that moment in time. Moreover, we trim long historical non churn sequence, in order to reduce non churn events.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "il8mQxW7mMwC"
      },
      "source": [
        "### Train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-RU8QkFWgx0"
      },
      "outputs": [],
      "source": [
        "target_column = 'churn_regr'\n",
        "time_column = 'timestamp'\n",
        "time_series_identifier_column = 'uuid'\n",
        "available_at_forecast_columns = [\n",
        "  'timestamp', 'phone_duration',\t'internet_duration',\t\n",
        "  'no_service_calls']\n",
        "time_series_attribute_columns = [\n",
        "  'gender',\t'phone_services',\t'internet_services',\t\n",
        "  'contract_duration',\t'payment_method',\t'avg_monthly_bill']\n",
        "cols = [col for col in df_train_acc_inter_v2.columns if col not in \n",
        "        [time_column, time_series_identifier_column]]\n",
        "categorical_cols = df_train_acc_inter_v2.loc[:, cols].select_dtypes(exclude=\"number\").columns\n",
        "numerical_cols = df_train_acc_inter_v2.loc[:, cols].select_dtypes(include=\"number\").columns\n",
        "\n",
        "COLUMN_SPEC = {\n",
        "    time_column: 'timestamp',\n",
        "    target_column: 'churn_regr'\n",
        "}\n",
        "for col_i in numerical_cols:\n",
        "  COLUMN_SPEC[col_i] = \"numeric\"\n",
        "for col_i in categorical_cols:\n",
        "  COLUMN_SPEC[col_i] = \"categorical\"\n",
        "\n",
        "forecast_horizon = 3\n",
        "context_window = 6\n",
        "model_ts_display_name = \"timeseries-churn-prediction-model-regr\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yuyzRYA7WtB2"
      },
      "outputs": [],
      "source": [
        "model_ts_id = config['artifacts']['model_id']['long_term_churn_model_v2']\n",
        "if model_ts_id is None:\n",
        "  job_long_term = aiplatform.AutoMLForecastingTrainingJob(\n",
        "      display_name=f'job_train_model-{model_ts_display_name}-{TIMESTAMP}',\n",
        "      optimization_objective='minimize-rmse', #'minimize-rmse',#'minimize-quantile-loss', #'minimize-mae',    \n",
        "      column_specs = COLUMN_SPEC,\n",
        "  )\n",
        "\n",
        "  # This will take around an hour to run\n",
        "  model_ts = job_long_term.run(\n",
        "      dataset=DATASET_ACC_INTER_V2,\n",
        "      target_column=target_column,\n",
        "      time_column=time_column,\n",
        "      time_series_identifier_column=time_series_identifier_column,\n",
        "      available_at_forecast_columns=available_at_forecast_columns,\n",
        "      unavailable_at_forecast_columns=[target_column],\n",
        "      time_series_attribute_columns=time_series_attribute_columns,\n",
        "      forecast_horizon=forecast_horizon,\n",
        "      context_window=context_window,\n",
        "      data_granularity_unit=\"month\",\n",
        "      data_granularity_count=1,\n",
        "      weight_column=None,\n",
        "      budget_milli_node_hours=1000,\n",
        "      model_display_name=model_ts_display_name, \n",
        "      predefined_split_column_name=None\n",
        "  )\n",
        "else:\n",
        "  model_ts = aiplatform.Model(model_ts_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZaQnuCh4uPu",
        "outputId": "d6ed8f40-2be1-47c2-c4cb-ecc86c044f30"
      },
      "outputs": [],
      "source": [
        "print(\"Model evaluation performance: \")\n",
        "for model_ts_evaluation in model_ts.list_model_evaluations():\n",
        "  info_dict = model_ts_evaluation.to_dict()\n",
        "  for metric, value in info_dict['metrics'].items():\n",
        "    print(f\"  - Model {metric}: {value}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0fO7B5jjYsXK"
      },
      "outputs": [],
      "source": [
        "DATA_ACC_INTER_FOLDER_V2 = config['artifacts']['model_path']['long_term_churn_model_v2_folder']\n",
        "model_lt_results = config['data']['output_data']['file_path_long_term_churn_model_v2']\n",
        "gsc_output_folder = os.path.join(\"gs://\", BUCKET_NAME, DATA_ACC_INTER_FOLDER_V2, f\"predictions-{TIMESTAMP}\")\n",
        "\n",
        "df_ts_result = get_df_from_batch_predict(\n",
        "    CSV_PREDICT_ACC_INTER_V2, \n",
        "    gsc_output_folder, \n",
        "    model_ts_display_name,\n",
        "    BUCKET_NAME,\n",
        "    model_ts,\n",
        "    model_lt_results)\n",
        "\n",
        "# merge true label\n",
        "df_ts_result = df_ts_result.merge(\n",
        "    df_predict_acc_inter_v2[['timestamp', 'uuid', 'true_label']], \n",
        "    on=['timestamp', 'uuid'])\n",
        "\n",
        "# convert label and predict to prob values from 0 to 1\n",
        "df_ts_result['predicted_churn'] = 1/(1+np.exp(-df_ts_result['predicted_churn_regr']))\n",
        "df_ts_result['true_label'] = 1/(1+np.exp(-df_ts_result['true_label']))\n",
        "\n",
        "df_ts_result.sort_values(by=['uuid', 'timestamp'], inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQ6cpyFkTLeE"
      },
      "source": [
        "## Compare Sequence (V2) with Tabular (V1) approach"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "05IuouDE5iNu",
        "outputId": "b8de89a8-6565-4886-d2e4-1d80c9036d62"
      },
      "outputs": [],
      "source": [
        "cm_sequence_model = pd.crosstab(\n",
        "    df_ts_result['true_label'].round(0), \n",
        "    df_ts_result['predicted_churn'].round(0), \n",
        "    rownames=['Actual'], \n",
        "    colnames=['Predicted'], \n",
        "    margins=False\n",
        "    ).rename({0: 'retain', 1:'churn'}\n",
        "    ).rename({0: 'retain', 1:'churn'}, axis=1)\n",
        "\n",
        "precision_forecaster = round(\n",
        "    cm_sequence_model.loc['churn', 'churn'] / \n",
        "    cm_sequence_model.loc[:, 'churn'].sum(), 2)\n",
        "recall_forecaster = round(\n",
        "    cm_sequence_model.loc['churn', 'churn'] / \n",
        "    cm_sequence_model.loc['churn', :].sum(), 2)\n",
        "\n",
        "cm_sequence_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "qsJEhEKgRkWc",
        "outputId": "5fa2ef8d-4650-4152-bcfe-7009d8d41d70"
      },
      "outputs": [],
      "source": [
        "cm_tabular_model = pd.crosstab(\n",
        "    df_lt_result.loc[df_lt_result['time_horizon']==6, 'churn'].reset_index(drop=True), \n",
        "    df_lt_result.loc[df_lt_result['time_horizon']==6,'predicted_churn'].reset_index(drop=True),\n",
        "    rownames=['Actual'], \n",
        "    colnames=['Predicted'], \n",
        "    margins=False\n",
        "    ).rename({0: 'retain', 1:'churn'}\n",
        "    ).rename({0: 'retain', 1:'churn'}, axis=1)\n",
        "\n",
        "precision_tabular = round(\n",
        "    cm_tabular_model.loc['churn', 'churn'] / \n",
        "    cm_tabular_model.loc[:, 'churn'].sum(), 2)\n",
        "recall_tabular = round(\n",
        "    cm_tabular_model.loc['churn', 'churn'] / \n",
        "    cm_tabular_model.loc['churn', :].sum(), 2)\n",
        "\n",
        "cm_tabular_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FEdBrXiyTyKZ",
        "outputId": "1dba1305-9f63-41c5-8338-e0537531ed95"
      },
      "outputs": [],
      "source": [
        "print(\"** Comparison of two approaches\\n\")\n",
        "print(f\"Precision:\\n    \\\n",
        "Binary Classification {precision_tabular}\\\n",
        "\\n    Sequential {precision_forecaster}\\n\\n\")\n",
        "print(f\"Recall:\\n    \\\n",
        "Binary Classification {recall_tabular}\\\n",
        "\\n    Sequential {recall_forecaster}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHAC4XL6Z7XW"
      },
      "source": [
        "Comparison of two approaches show that sequential model trained with AutoML Forecasting yield even better results than model trained with AutoML Tabular. That is, sequence model shows values of 0.67 Precision and 1.00 Recall while binary classification approach shows values of 0.51 Precision and 0.69 Recall. One of the reasons could be that tabular model was trained on more imbalanced dataset because it predicts values for six months in advance, while sequence model predicts values for just 6th month in a future. Nevertheless, this exercise shows that it is recommended to try both approaches in order to evaluate which one will bring better results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2M90FWcDUWMr"
      },
      "source": [
        "## Uplift modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQdj8kAfMAh7"
      },
      "source": [
        "Uplifting model is used to predict how much gain in probability we can get if we offer users some treatment in comapre to scenario without treatment. For this case, we are going to use once more time AutoMLTabular (Vertex AI) service. Like in previous cases, data is imbalanced with smaller number of users that retain after teatment is subscribed. The final goal of this section is to find treatemnts that generate highest gain in probability of retaining each individual users."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5l5pxA6I6h68"
      },
      "outputs": [],
      "source": [
        "COLUMN_SPEC.update({'treatment':'categorical'})\n",
        "target_column = 'churn_after_treatment'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVhGkRjUoI1X"
      },
      "source": [
        "### Train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jhg9Ch266RSN"
      },
      "outputs": [],
      "source": [
        "model_uplift_id = config['artifacts']['model_id']['uplift_model']\n",
        "model_uplift_name = \"uplift-churn-prediction-model\"\n",
        "\n",
        "if model_uplift_id is None:\n",
        "  job_tabular = aiplatform.AutoMLTabularTrainingJob(\n",
        "      display_name=f'job_train_model-{model_uplift_name}-{TIMESTAMP}',\n",
        "      optimization_prediction_type='classification',\n",
        "      optimization_objective='maximize-au-prc',\n",
        "      column_specs=COLUMN_SPEC\n",
        "  )\n",
        "\n",
        "  model_uplift = job_tabular.run(\n",
        "      dataset = DATASET_ACCOUNT,\n",
        "      target_column = target_column,\n",
        "      training_fraction_split = 0.7,\n",
        "      validation_fraction_split = 0.15,\n",
        "      test_fraction_split = 0.15,\n",
        "      budget_milli_node_hours=1000,\n",
        "      model_display_name=model_uplift_name,\n",
        "  )\n",
        "else:\n",
        "  model_uplift = aiplatform.Model(model_uplift_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LG_S3-mEoLDj"
      },
      "source": [
        "### Analyse model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 809
        },
        "id": "_XKrE7u5Nua4",
        "outputId": "af2a6648-8e2c-400f-bdcf-61a106368664"
      },
      "outputs": [],
      "source": [
        "_ = model_evaluation(model_uplift, highlight_feature='treatment')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPx3HtmsA_8T"
      },
      "source": [
        "Feature importance graph shows that treatment feature are one of the most important ones. Therefore, model is relatively sensitivity towards this feature, which allows us to use it for defining uplift gains of different scenarious."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuFXRSivoN1v"
      },
      "source": [
        "### Made predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACcHW_cPB73_"
      },
      "source": [
        "In order to find best strategies, we are going to calculate gain in probability of users retaining by subtractig probability of treatment scenario with case when treatment is not offered. Hence, first we populate treatment feature with None values to calculate predictions for non treatment scenario, and after that we are going to repeat the same for each strategy but this time populating treatment feature with the name of relevant treat. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8kTKg2xvvukh"
      },
      "outputs": [],
      "source": [
        "# for name in ('none', 'discount', 'free_device', 'upg_packet'):\n",
        "#     df = read_csv_from_bucket(CSV_PREDICT_ACCOUNT)\n",
        "#     df['treatment'] = name\n",
        "#     write_csv_to_bucket(df.to_csv(index=False), f\"account_predict_uplift_{name}.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KUsFGLVKN6jy"
      },
      "outputs": [],
      "source": [
        "model_uplift_paths_outputs = config['data']['output_data']['files_paths_uplift_model']\n",
        "\n",
        "dfs_uplift_result_list = []\n",
        "\n",
        "for name in model_uplift_paths_outputs.keys():\n",
        "\n",
        "    model_uplift_path = model_uplift_paths_outputs[name]\n",
        "\n",
        "    csv_file = os.path.join(\n",
        "        \"gs://\", BUCKET_NAME, config['data']['input_data']['files_names_uplift_model'][name]\")\n",
        "\n",
        "    DATA_UPLIFT_FOLDER = f\"mn-model-uplift-{name}-output\"\n",
        "    gsc_output_folder = os.path.join(\"gs://\", BUCKET_NAME, DATA_UPLIFT_FOLDER, f\"predictions-uplift-{TIMESTAMP}\")\n",
        "\n",
        "    df_uplift_result = get_df_from_batch_predict(\n",
        "        csv_file, \n",
        "        gsc_output_folder, \n",
        "        model_uplift_name,\n",
        "        BUCKET_NAME,\n",
        "        model_uplift,\n",
        "        model_uplift_path)\n",
        "\n",
        "    dfs_uplift_result_list.append(df_uplift_result)\n",
        "\n",
        "df_uplift = pd.concat(dfs_uplift_result_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0NCE7eMwJX0"
      },
      "outputs": [],
      "source": [
        "# for each user extract information about probability of retain after each treatment\n",
        "df_treatment = df_uplift.groupby(\n",
        "    ['uuid', 'treatment'], as_index=False)['churn_after_treatment_0_scores'].first().rename(\n",
        "    columns={'churn_after_treatment_0_scores': 'treatment_prob_of_retention'})\n",
        "\n",
        "# extract no treatment cases\n",
        "df_no_treatment = df_uplift.loc[df_uplift['treatment'].isna(), ['uuid',\t'churn_after_treatment_0_scores']].rename(\n",
        "    columns={'churn_after_treatment_0_scores': 'no_treatment_prob_of_retention'})\n",
        "\n",
        "# compare treaetment with no treatment cases\n",
        "df_compare_treatment = df_treatment.merge(df_no_treatment, on=['uuid'])\n",
        "df_compare_treatment['difference'] = df_compare_treatment['treatment_prob_of_retention'] - \\\n",
        "    df_compare_treatment['no_treatment_prob_of_retention']\n",
        "\n",
        "df_uplift_uid = df_uplift.loc[:, \n",
        "      ['uuid', 'treatment', 'churn_after_treatment_0_scores']\n",
        "    ].fillna('None').sort_values(\n",
        "      ['treatment', 'uuid']\n",
        "    ).pivot_table(\n",
        "      index = ['uuid'], \n",
        "      columns = 'treatment', \n",
        "      values = 'churn_after_treatment_0_scores'\n",
        "    )\n",
        "\n",
        "# compute uplift between adjacent values of treatments in a table\n",
        "for col_next, col_prev in zip(df_uplift_uid.columns[:0:-1], df_uplift_uid.columns[-2::-1]): #[:-1:][::-1]):\n",
        "    df_uplift_uid[col_next] = df_uplift_uid[col_next] - df_uplift_uid[col_prev]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAFXy7NwWcUs"
      },
      "source": [
        "Selecting best treatments for each user.\n",
        "- First, filter out users that probably doesn't need any treatment, which will most certainly retain\n",
        "- Second, filter out users that are lost cause, that probably could not be retain or it would be too expensive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "CIlYFg9dUhXk",
        "outputId": "e2ec3ea3-a290-43e6-ae3d-12bb50644033"
      },
      "outputs": [],
      "source": [
        "df_highest_gain_treatment = df_compare_treatment.sort_values(\n",
        "        by=['uuid', 'difference'], ascending=False\n",
        "    ).loc[\n",
        "        lambda df: df['difference']>=0\n",
        "    ].drop_duplicates(['uuid'])\n",
        "\n",
        "df_highest_gain_treatment.loc[\n",
        "    lambda df: (df['no_treatment_prob_of_retention'] <0.95) & \n",
        "    (df['treatment_prob_of_retention']>0.2), ['uuid','treatment']].sample(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 577
        },
        "id": "HbHHc93JmSDK",
        "outputId": "b0931e32-0555-421b-f078-dbb25f25787e"
      },
      "outputs": [],
      "source": [
        "import waterfall_chart\n",
        "\n",
        "uuid = df_highest_gain_treatment.iloc[0, 0] \n",
        "waterfall_chart.plot(df_uplift_uid.loc[uuid].index, \n",
        "                      df_uplift_uid.loc[uuid].values)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcU0GH5ZkEmi"
      },
      "source": [
        "### Budget optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WI4zX_IoDYUq"
      },
      "source": [
        "At the end, if there is constrained budget for each of treatment, we may implement MIP (mixed integer programming) to optimise spendings regarding the expected gain from uplift outcomes. This is out of the scope of this notebook, but once we got all predictins we may consider to optimise following setup\n",
        "\n",
        "\n",
        "$$\n",
        "max \\sum\\sum X_{j,i}*P_{j,i}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6XtFUYWdKy8"
      },
      "source": [
        "subject to:\n",
        "\n",
        "> $$B_{discount} > \\sum X_{discount,i}*costTreatments_{discount}$$\n",
        "> $$B_{upgrade} > \\sum X_{upgrade,i}*costTreatments_{upgrade}$$\n",
        "> $$B_{freedevice} > \\sum X_{freedevice,i}*costTreatments_{freedevice}$$\n",
        "> $$X_{discount,i} + X_{free device,i} + X_{upgrade,i} + X_{none,i} = 1$$\n",
        "> $$ 𝑃_{𝑗,𝑖} \\in [ \\, 0,1 ] \\,$$\n",
        "> $$X_{j,i} \\in \\{ 0,1 \\}$$\n",
        "> $$j \\in \\{ freedevice, upgrade, discount, none \\}$$\n",
        "> $$i \\in [ \\, 1 ... numberOfUsers ] \\,$$\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERpc3wuUdL_8"
      },
      "source": [
        "where\n",
        "\n",
        "\n",
        "- $X_{j,i}$ - represent decision binary varaibale\n",
        "- $P_{j,i}$ - probability of non churning for user i if we implement treatment j\n",
        "- $costTreatments_j$ - cost of implementing treatment j\n",
        "- $B_j$ - budget for treatment j"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CU-Xxvddp5O"
      },
      "source": [
        "## Advance: Identify of triggers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2cUWidbwzeE"
      },
      "source": [
        "Identification of churn triggers in a system could lead to numerous benefits, likewise: determining bottlenecks in user funnel, detecting counterintuitive parts of the product, steep learning curve, unresponsive customer support, bad quality of product etc. In fact, churn analytics could identify trigger parts of the system that need to be improved. Here we will summarize the approach of revealing users' triggers in a system through implementation of sentimental models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Y-19aMudstL"
      },
      "source": [
        "### Sentiment model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEBSeRpzwqgW"
      },
      "source": [
        "Creation of an user-generated  synthetic dataset with enough data to train the model is a challenging problem itself and therefore we decided to use publicly available “Crowdflower Claritin-Twitter” dataset for this purpose. This type of approach uses similar logic as transfer learning, but in this case we are not taking weights of models but dataset of similar context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F0arZHELj5mr"
      },
      "outputs": [],
      "source": [
        "CSV_SENTIMENT_FILE =  config['data']['input_data']['file_path_sentiment_train']\n",
        "SENTIMENT_MAX = 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wbFFH7VmAd2f"
      },
      "outputs": [],
      "source": [
        "model_sentiment_id = config['artifacts']['model_id']['sentiment_model']\n",
        "model_sentiment_name = \"sentiment-claritin-demo-model\"\n",
        "\n",
        "if model_uplift_id is None:\n",
        "\n",
        "    sentiment_dataset = aiplatform.TextDataset.create(\n",
        "        display_name=\"Crowdflower Claritin-Twitter\" + \"_\" + TIMESTAMP,\n",
        "        gcs_source=[CSV_SENTIMENT_FILE],\n",
        "        import_schema_uri=aiplatform.schema.dataset.ioformat.text.sentiment,\n",
        "    )\n",
        "\n",
        "    job = aiplatform.AutoMLTextTrainingJob(\n",
        "      display_name=model_sentiment_name+\"-\" + TIMESTAMP,\n",
        "      prediction_type=\"sentiment\",\n",
        "      sentiment_max=SENTIMENT_MAX,\n",
        "    )\n",
        "\n",
        "    model_sentiment = job.run(\n",
        "      dataset=sentiment_dataset,\n",
        "      model_display_name=model_sentiment_name+\"-\" + TIMESTAMP,\n",
        "      training_fraction_split=0.8,\n",
        "      validation_fraction_split=0.1,\n",
        "      test_fraction_split=0.1,\n",
        "    )\n",
        "else:\n",
        "    model_sentiment = aiplatform.Model(model_sentiment_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "la7OKikWxDrY"
      },
      "source": [
        "Once we trained model we may use it to filter out the good from bad sentiment. For this purposes, we are going to use synthetically generated users email written customer support of imaginary telco company."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uL5o5hZxJU9P"
      },
      "outputs": [],
      "source": [
        "input_file = os.path.join(\"gs://\", BUCKET_NAME, config['data']['input_data']['file_name_sentiment_predict'])\n",
        "output_file = config['data']['output_data']['file_path_sentiment']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uorcnErDJk6c"
      },
      "outputs": [],
      "source": [
        "if not output_file:\n",
        "    DATA_SENTIMENT_FOLDER = config['artifacts']['model_path']['sentiment_model_folder']\n",
        "    gsc_output_folder = os.path.join(\"gs://\", BUCKET_NAME, DATA_SENTIMENT_FOLDER, f\"predictions-sentiment-{TIMESTAMP}\")\n",
        "\n",
        "\n",
        "    batch_predict_job = model_sentiment.batch_predict(\n",
        "              gcs_source=input_file,\n",
        "              instances_format=\"jsonl\",\n",
        "              gcs_destination_prefix=gsc_output_folder,\n",
        "              predictions_format=\"jsonl\",\n",
        "              job_display_name=f\"job-batch_predict-{model_sentiment_name}-{TIMESTAMP}\",\n",
        "              sync=True\n",
        "            )\n",
        "    batch_predict_job.wait()\n",
        "\n",
        "    for row in batch_predict_job.iter_outputs():\n",
        "        output_file = row.name\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1nVnB2dok6qb"
      },
      "outputs": [],
      "source": [
        "json_data = bucket_to_bytes(output_file, bucket_name=BUCKET_NAME)\n",
        "df_sentiment_output = pd.read_json(path_or_buf=json_data, lines=True)\n",
        "df_sentiment_output['prediction'] = df_sentiment_output['prediction'].apply(lambda x: x['sentiment'])\n",
        "\n",
        "for i in range(df_sentiment_output.shape[0]):\n",
        "    txt_data = bucket_to_bytes(file_name=df_sentiment_output.iloc[i][0]['content'])\n",
        "    df_sentiment_output.iloc[i, 0] = txt_data.read().decode('UTF-8')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnHXXq54xfv1"
      },
      "source": [
        "Results of selected emails with bad sentiment are following:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XSCFYE9heBsR",
        "outputId": "0719fa18-7df0-406a-8401-aa4517a1ccc9"
      },
      "outputs": [],
      "source": [
        "df_sentimental_select = df_sentiment_output.loc[lambda df: df['prediction']>2, ['instance']]\n",
        "for i in range(df_sentimental_select.shape[0]):\n",
        "    print(f\"Email {i}: \", df_sentimental_select.iloc[i, 0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJx5KmIZxnUo"
      },
      "source": [
        "Next we are going to label our dataset of bad sentiment by comparing vector representation of words from emails with vector representation of triggers. In general, for this purposes we should use more sofisticate metrics and to filter out stop words, but just for sace of simplicity we are going to keep it simple and only count for words that have higher similarity than 0.65 to trigger words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NIkMKmAmfcx7"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en_core_web_md')\n",
        "triggers = ['price', 'quality']\n",
        "trigger_tokens = nlp('price quality')\n",
        "symilarity_threshold = 0.65\n",
        "results = {}\n",
        "\n",
        "for trigger in trigger_tokens:\n",
        "    score = 0\n",
        "    words = 0\n",
        "    df_sentimental_select[trigger.text] = 0\n",
        "    for i in range(df_sentimental_select.shape[0]):\n",
        "        \n",
        "        tokens = nlp(df_sentimental_select.iloc[i,0])\n",
        "\n",
        "        for token in tokens:\n",
        "            similarity_score = token.similarity(trigger)\n",
        "            if  similarity_score > symilarity_threshold:\n",
        "                score += similarity_score\n",
        "                words += 1\n",
        "        df_sentimental_select.loc[lambda df: df.index[i],trigger.text]  = round(score/words,2) if words>0 else 0.\n",
        "\n",
        "df_sentimental_select[triggers] = df_sentimental_select[triggers].div(df_sentimental_select[triggers].sum(axis=1), axis=0).fillna(0).round(2)\n",
        "df_sentimental_select['external'] = 1 - df_sentimental_select[triggers].sum(axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzBiqL_2x8XU"
      },
      "source": [
        "As result, outcome of labeling are following:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "hI9MQiLPb-jM",
        "outputId": "6d4efe6f-b146-4965-90ee-9162c3b6ca36"
      },
      "outputs": [],
      "source": [
        "df_sentimental_select"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "toc_visible": true
    },
    "interpreter": {
      "hash": "22fac32b615108da171497d01b47d1be1bcb782a3d8a035a44469f5caba893bd"
    },
    "kernelspec": {
      "display_name": "Python 3.7.0 ('churn-prediction-vertex-ai')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
