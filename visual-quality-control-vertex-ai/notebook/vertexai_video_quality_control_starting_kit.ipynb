{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import json\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "from typing import Optional, Dict, Sequence, Tuple\n",
    "\n",
    "import base64\n",
    "import csv\n",
    "\n",
    "import math\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "import random\n",
    "\n",
    "from google.cloud import storage, aiplatform\n",
    "from google.cloud.aiplatform.gapic.schema import predict\n",
    "from google.oauth2 import service_account"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify your Google Cloud Project and region to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcloud_project = 'gd-gcp-rnd-visual-quality-ctrl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcloud_location = 'us-central1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcloud_storage_location = 'us'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Main functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video I/O functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to convert a video to a list of images and back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_video(input_video_path, output_dir, sample_rate=60):\n",
    "    '''\n",
    "    Splits video into frames\n",
    "    input_video_path: Path to Input Video\n",
    "    output_dir: Path to Directory for Storing Image Frames\n",
    "    sample_rate: Sampling rate to extract frames from video (60 on a 30 fps video = 1 frame every 2 s)\n",
    "    '''\n",
    "    video_capture = cv2.VideoCapture(input_video_path)\n",
    "    result = []\n",
    "    \n",
    "    frame_id = 0\n",
    "    while True:\n",
    "        success, image = video_capture.read()\n",
    "        if not success:\n",
    "            break\n",
    "        if (frame_id % sample_rate) == 0:\n",
    "            image_name = f\"frame_{frame_id}\"\n",
    "            save_path = os.path.join(output_dir, f\"{image_name}.jpg\")\n",
    "            cv2.imwrite(save_path, image)\n",
    "            result.append((image_name, save_path))\n",
    "        frame_id += 1\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_video_from_frames(frames, save_path, fps=60):\n",
    "    '''\n",
    "    Creates a video from frames\n",
    "    frames: list of image name, image path\n",
    "    save_path: Path/Directory to save the rendered video to.\n",
    "    '''\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "\n",
    "    out = None\n",
    "\n",
    "    for frame_name, frame_path in tqdm(frames):\n",
    "        frame = cv2.imread(frame_path)\n",
    "        if out is None:\n",
    "            out = cv2.VideoWriter(save_path, fourcc, fps, (frame.shape[1], frame.shape[0]))\n",
    "        out.write(frame.astype('uint8'))\n",
    "\n",
    "    out.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image cropping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bounding boxes of different objects may have different size. We crop the objects, pad it to the square and resize to have the same size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_WIDTH = 256\n",
    "IMAGE_HEIGHT = 256\n",
    "PAD_COLOR = (0, 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_n_resize_image(img, bbox, size, padColor=0):\n",
    "    # crop images\n",
    "    crop = img[bbox[1]:bbox[1] + bbox[3], bbox[0]:bbox[0] + bbox[2]].copy()\n",
    "    \n",
    "    # cropped image size\n",
    "    h, w = crop.shape[:2]\n",
    "    # designed crop image sizes\n",
    "    sh, sw = size\n",
    "\n",
    "    # interpolation method\n",
    "    if h > sh or w > sw: # shrinking image\n",
    "        interp = cv2.INTER_AREA\n",
    "    else: # stretching image\n",
    "        interp = cv2.INTER_CUBIC\n",
    "\n",
    "    # aspect ratio of image\n",
    "    aspect = w/h \n",
    "\n",
    "    # compute scaling and pad sizing\n",
    "    if aspect > 1: # horizontal image\n",
    "        new_w = sw\n",
    "        new_h = np.round(new_w/aspect).astype(int)\n",
    "        pad_vert = (sh-new_h)/2\n",
    "        pad_top, pad_bot = np.floor(pad_vert).astype(int), np.ceil(pad_vert).astype(int)\n",
    "        pad_left, pad_right = 0, 0\n",
    "    elif aspect < 1: # vertical image\n",
    "        new_h = sh\n",
    "        new_w = np.round(new_h*aspect).astype(int)\n",
    "        pad_horz = (sw-new_w)/2\n",
    "        pad_left, pad_right = np.floor(pad_horz).astype(int), np.ceil(pad_horz).astype(int)\n",
    "        pad_top, pad_bot = 0, 0\n",
    "    else: # square image\n",
    "        new_h, new_w = sh, sw\n",
    "        pad_left, pad_right, pad_top, pad_bot = 0, 0, 0, 0\n",
    "\n",
    "    # set pad color\n",
    "    if len(img.shape) == 3 and not isinstance(padColor, (list, tuple, np.ndarray)): # color image but only one color provided\n",
    "        padColor = [padColor] * 3\n",
    "\n",
    "    # scale and pad\n",
    "    scaled_img = cv2.resize(crop, (new_w, new_h), interpolation=interp)\n",
    "    scaled_img = cv2.copyMakeBorder(scaled_img, pad_top, pad_bot, pad_left, pad_right, borderType=cv2.BORDER_CONSTANT, value=padColor)\n",
    "\n",
    "    return scaled_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-object tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DETECTION_RATE = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DST_THRESHOLD = 100\n",
    "UPD_DST_THRESHOLD = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DETECTION_RATE - rate to use object detection X frames (mainly to detect new objects that enters the frame)\n",
    "\n",
    "DST_THRESHOLD - distance threshold to consider two boxes belong to the same object (to not count twice objects that were already detected previosly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_center(box):\n",
    "    '''\n",
    "    box: a bounding box in X, Y, W, H format\n",
    "    returns a center of a bounding box\n",
    "    '''\n",
    "    return box[0] + box[2] / 2, box[1] + box[3] / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTracker:\n",
    "    '''\n",
    "    tracks bounding boxes for detected objects through the video\n",
    "    new objects should be initializes using add_boxes function (takes object detection results as an input)\n",
    "    '''\n",
    "    def __init__(self, inactive_thresh=2, upd_area_thresh=2.0, dst_thresh=100, upd_dst_thresh=30,\n",
    "                 tracker_func=cv2.legacy.TrackerKCF_create):\n",
    "        '''\n",
    "        inactive_thresh: inactive objects threshold - if object is not present on the frame in {inactive_thresh}\n",
    "          continious updates, drop it\n",
    "        upd_area_thresh: a lower bound of are fraction to replace a box if a larger one was detected for\n",
    "          the same object\n",
    "        dst_thresh: distance threshold to consider two boxes belong to the same object\n",
    "        tracker_func: OpenCV tracker creation function for individual objects\n",
    "        '''\n",
    "        self.trackers = []\n",
    "        self.boxes = []\n",
    "        self.inactive_time = []\n",
    "        self.inactive_thresh = inactive_thresh\n",
    "        self.upd_area_thresh = upd_area_thresh\n",
    "        self.dst_thresh = dst_thresh\n",
    "        self.upd_dst_thresh = upd_dst_thresh\n",
    "        self.tracker_func = tracker_func\n",
    "        \n",
    "        \n",
    "    def get_objects(self):\n",
    "        '''\n",
    "        returns a list of bounding boxes for all objects on the frame\n",
    "        '''\n",
    "        result = []\n",
    "        for obj_id, box in enumerate(self.boxes):\n",
    "            if self.inactive_time[obj_id] == 0:\n",
    "                result.append((obj_id, box))\n",
    "        return result\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_dist(box1, box2):\n",
    "        '''\n",
    "        returns distance between centroids of bounding boxes\n",
    "        box1, box2: bounding boxes in X, Y, W, H format\n",
    "        '''\n",
    "        cx1, cy1 = get_center(box1)\n",
    "        cx2, cy2 = get_center(box2)\n",
    "        dist = math.hypot(cx1 - cx2, cy1 - cy2)\n",
    "        return dist\n",
    "    \n",
    "    \n",
    "    def _create_tracker(self, box, frame):\n",
    "        '''\n",
    "        initializes a new tracker for a newly found object\n",
    "        box: a bounding box in X, Y, W, H format\n",
    "        frame: the entire frame image \n",
    "        '''\n",
    "        tracker = self.tracker_func()\n",
    "        tracker.init(frame, box)\n",
    "        return tracker\n",
    "        \n",
    "        \n",
    "    def add_boxes(self, boxes, frame):\n",
    "        '''\n",
    "        adds all new detected boxes to the tracker (creates trackers for them);\n",
    "        ignores boxes for already tracked objects\n",
    "        boxes: a list of boxes in X, Y, W, H format\n",
    "        frame: the entire frame image \n",
    "        '''\n",
    "        for box in boxes:\n",
    "            found = False\n",
    "            for obj_id, existing_box in enumerate(self.boxes):\n",
    "                if self.inactive_time[obj_id] != 0:\n",
    "                    continue\n",
    "                centroid_dist = self._get_dist(box, existing_box)\n",
    "                if centroid_dist < self.dst_thresh:\n",
    "                    # the new box and the existing one represent the same object\n",
    "                    \n",
    "                    if (centroid_dist < self.upd_dst_thresh) or \\\n",
    "                            (box[2] * box[3] >= existing_box[2] * existing_box[3] * self.upd_area_thresh):\n",
    "                        # replace the existing one with the new one, if the existing one is too far from the centroid\n",
    "                        # (probably the existing box slightly shifted because of inaccurate tracking)\n",
    "                        # OR the new box is significantly bigger (probably means it is entering the frame)\n",
    "                        self.trackers[obj_id] = self._create_tracker(box, frame)\n",
    "                        self.inactive_time[obj_id] = 0\n",
    "                        self.boxes[obj_id] = box\n",
    "                    found = True\n",
    "                    break\n",
    "            if not found:\n",
    "                # this box is a new one -> create a new tracker for this object\n",
    "                self.trackers.append(self._create_tracker(box, frame))\n",
    "                self.inactive_time.append(0)\n",
    "                self.boxes.append(box)\n",
    "                    \n",
    "\n",
    "    def update(self, frame):\n",
    "        '''\n",
    "        updates existing object positions based on a new frame\n",
    "        frame: a next frame image\n",
    "        '''\n",
    "        for obj_id, tracker in enumerate(self.trackers):\n",
    "            if self.inactive_time[obj_id] < self.inactive_thresh:\n",
    "                success, bbox = tracker.update(frame)\n",
    "                if success:\n",
    "                    self.inactive_time[obj_id] = 0\n",
    "                    self.boxes[obj_id] = bbox\n",
    "                else:\n",
    "                    self.inactive_time[obj_id] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to make a demo video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_info(img, info):\n",
    "    '''\n",
    "    adds text info to the bottom of the image\n",
    "    img: an image to add info\n",
    "    info: list of key, value pairs to print on the image\n",
    "    '''\n",
    "    H, W = img.shape[:2]\n",
    "    \n",
    "    cols = 2\n",
    "    rows = (len(info) + cols - 1) // cols\n",
    "    scale = 1.4\n",
    "    color = (0, 0, 0)\n",
    "    thickness = 2\n",
    "    \n",
    "    rh = 50 + rows * 70\n",
    "    rw = W\n",
    "    text_box = np.ones((rh, rw, 3), dtype=np.uint8) * 255\n",
    "    \n",
    "    for (i, (k, v)) in enumerate(info):\n",
    "        c = i // rows\n",
    "        r = i % rows\n",
    "        x = 50 + ((rw // cols - 100) * c)\n",
    "        y = 70 + r * 70\n",
    "        text = \"{}: {}\".format(k, v)\n",
    "        cv2.putText(text_box, text, (x, y), cv2.FONT_HERSHEY_SIMPLEX, scale, color, thickness)\n",
    "        \n",
    "    return cv2.vconcat([img, text_box])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_frames_with_tracking_boxes(frames, tracking_ad_results, output_result_dir):\n",
    "    '''\n",
    "    adds bounding boxes and text info to the video frames, saves it to a new folder\n",
    "    frames: a list of frames with their names (each frame is represented as a pair: a name and a path to the image)\n",
    "    tracking_ad_results: dict with all tracking and anomaly detection results; keys are frame names and values are\n",
    "        lists of all bounding boxes, each bounding box is a tuple (idx, box, is_anomaly, confidence)\n",
    "            idx - object id, persisted between frames\n",
    "            box - bounding box in X, Y, W, H format\n",
    "            is_anomaly - boolean flag for anomaly classification\n",
    "            confidence - AWS Lookout for Vision confidence of resulting verdict\n",
    "    output_result_dir: a directory to save resulting video frames\n",
    "    '''\n",
    "    result_frames = []\n",
    "    objects_total = set()\n",
    "    anomalies_total = set()\n",
    "    for frame_name, frame_path in tqdm(frames):\n",
    "        if frame_name not in tracking_ad_results:\n",
    "            break\n",
    "        objects_frame = set()\n",
    "        anomalies_frame = set()\n",
    "        \n",
    "        frame = cv2.imread(frame_path)\n",
    "        boxes = tracking_ad_results[frame_name]\n",
    "\n",
    "        for idx, box, is_anomaly, confidence in boxes:\n",
    "            x, y, w, h = box\n",
    "            x, y, w, h = int(x), int(y), int(w), int(h)\n",
    "            if confidence is None:\n",
    "                color = (255, 0, 0)\n",
    "                box_name = str(idx)\n",
    "            else:\n",
    "                anomaly_prob = confidence if is_anomaly else 1 - confidence\n",
    "                color = (0, 255 * (1 - anomaly_prob), 255 * anomaly_prob)\n",
    "                box_name = '{} {} {:.1f}'.format(idx, \"A\" if is_anomaly else \"N\", confidence)\n",
    "            cv2.putText(frame, box_name, (x, y - 15), cv2.FONT_HERSHEY_PLAIN, 2, color, 2)\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), color, 3)\n",
    "            objects_total.add(idx)\n",
    "            objects_frame.add(idx)\n",
    "            if is_anomaly:\n",
    "                anomalies_total.add(idx)\n",
    "                anomalies_frame.add(idx)\n",
    "        \n",
    "        info = [\n",
    "            (\"#objects total\", len(objects_total)),\n",
    "            (\"#anomalies total\", len(anomalies_total)),\n",
    "            (\"anomalies percentage total\", '{:.2f}'.format(len(anomalies_total) / len(objects_total) * 100)),\n",
    "            (\"#objects in the frame\", len(objects_frame)),\n",
    "            (\"#anomalies in the frame\", len(anomalies_frame)),\n",
    "            (\"anomalies percentage in the frame\", '{:.2f}'.format(len(anomalies_frame) / len(objects_frame) * 100)),\n",
    "        ]\n",
    "        frame = add_info(frame, info)\n",
    "\n",
    "        save_path = os.path.join(output_result_dir, f'{frame_name}.jpg')\n",
    "        cv2.imwrite(save_path, frame)\n",
    "\n",
    "        result_frames.append((frame_name, save_path))\n",
    "    return result_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Define I/O locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcs_bucket = 'gd-rnd-visual-quality-ctrl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcs_folder = 'cv_anomaly'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcs_input_video_path = os.path.join('s3://', gcs_bucket, gcs_folder, 'input_video', 'boxes.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workdir = '../data/boxes/'\n",
    "os.makedirs(workdir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_VIDEO_DOWNLOAD_LINK = 'https://www.dropbox.com/s/pot5874yqh11f7w/boxes.mp4?dl=1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_video_path = os.path.join(workdir, 'boxes.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_directory = os.path.join(workdir, 'output')\n",
    "os.makedirs(output_directory, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_video_frames_dir = os.path.join(output_directory, 'input_frames')\n",
    "os.makedirs(output_frames_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_result_dir = os.path.join(output_directory, 'result_frames')\n",
    "os.makedirs(output_result_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_video_path = os.path.join(output_directory, 'pipeline_demo.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_image(image_name):\n",
    "    image = cv2.imread(os.path.join(input_video_frames_dir, f'{image_name}.jpg'))\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the input video to working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -O \"{input_video_path}\" \"{INPUT_VIDEO_DOWNLOAD_LINK}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Split input video into frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "frames = split_video(input_video_path, input_video_frames_dir, sample_rate=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format datasets in VertexAI format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We labeled two separate datasets for object detection and image (anomaly) classification based on our input video. The dataset for object detection contains bounding boxes for every object in the video frames. The image classification dataset extends the object detection dataset, adding object IDs (objects have the same id across different frames) and class labels (normal/anomaly)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a GCS bucket to store images and manifests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_client = storage.Client(project=gcloud_project)\n",
    "\n",
    "bucket = storage_client.bucket(gcs_bucket)\n",
    "bucket = storage_client.create_bucket(bucket, location=gcloud_storage_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### I/O locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_annotations_path = '../resources/boxes_annotations/manual_detection_annotations.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_manifest_path = os.path.join(workdir, 'detection_annotations_gc.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcs_subfolder = os.path.join(gcs_folder, 'detection_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_images_folder = os.path.join(gcs_subfolder, 'images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcs_detection_mainfest_path = os.path.join(gcs_subfolder, 'annotations.jsonl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Read manual annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 5 {manual_annotations_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(manual_annotations_path) as f:\n",
    "    manual_annotations = list(csv.DictReader(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cast numeric fields to float\n",
    "numeric_fields = ['bbox_x', 'bbox_y', 'bbox_width', 'bbox_height']\n",
    "\n",
    "for annotation in manual_annotations:\n",
    "    for col, value in annotation.items():\n",
    "        if col in numeric_fields:\n",
    "            annotation[col] = float(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assign every frame to only one split (train / validation / test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_name_to_id = {\n",
    "    annotation['image_name']: int(annotation['image_name'].split('_')[1].split('.')[0]) \n",
    "    for annotation in manual_annotations\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_frames = list(sorted(set(image_name_to_id.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_name_to_data_split = {}\n",
    "for image_name, frame_id in image_name_to_id.items():\n",
    "    pos = sorted_frames.index(frame_id) / len(sorted_frames)\n",
    "    if pos < 0.7:\n",
    "        data_split = 'training'\n",
    "    elif pos < 0.85:\n",
    "        data_split = 'validation'\n",
    "    else:\n",
    "        data_split = 'test'\n",
    "    img_name_to_data_split[image_name] = data_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a manifest file in VertexAI format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_height, img_width = read_image(frames[0][0]).shape[:2]\n",
    "img_width, img_height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_to_annotations = defaultdict(list)\n",
    "for annotation in manual_annotations:\n",
    "    img_to_annotations[annotation['image_name']].append(annotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc_annotations = []\n",
    "for image_name, image_annotations in img_to_annotations.items():\n",
    "    gcs_path = os.path.join('gs://', gcs_bucket, upload_images_folder, image_name)\n",
    "    boxes_annotations = []\n",
    "    \n",
    "    for manual_annotation in image_annotations:\n",
    "        x = manual_annotation['bbox_x']\n",
    "        y = manual_annotation['bbox_y']\n",
    "        w = manual_annotation['bbox_width']\n",
    "        h = manual_annotation['bbox_height']\n",
    "        \n",
    "        annotation = {\n",
    "            \"displayName\": \"object\",\n",
    "            \"xMin\": min(1, x / img_width),\n",
    "            \"xMax\": min(1, (x + w) / img_width),\n",
    "            \"yMin\": min(1, y / img_height),\n",
    "            \"yMax\": min(1, (y + h) / img_height),\n",
    "        }\n",
    "        boxes_annotations.append(annotation)\n",
    "        \n",
    "    gc_annotations.append({\n",
    "        \"imageGcsUri\": gcs_path,\n",
    "        \"boundingBoxAnnotations\": boxes_annotations,\n",
    "        \"dataItemResourceLabels\": {\n",
    "            \"aiplatform.googleapis.com/ml_use\": img_name_to_data_split[image_name]\n",
    "        }\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manifest_lines = [json.dumps(annotation) + '\\n' for annotation in gc_annotations]\n",
    "with open(output_manifest_path, 'wt') as f:\n",
    "    f.writelines(manifest_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Upload images and manifest to gcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_images = list(img_name_to_data_split.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_client = storage.Client(project=gcloud_project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = storage_client.bucket(gcs_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_name in tqdm(input_images):\n",
    "    image_path = os.path.join(output_frames_dir, image_name)\n",
    "    destination_path = os.path.join(upload_images_folder, image_name)\n",
    "    blob = bucket.blob(destination_path)\n",
    "    blob.upload_from_filename(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bucket.blob(gcs_detection_mainfest_path).upload_from_filename(output_manifest_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anomaly classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I/O locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_annotations_path = '../resources/boxes_annotations/manual_cls_annotations.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_manifest_path = os.path.join(workdir, 'classification_annotations_gc.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_frames_dir = os.path.join(output_directory, 'cls_frames')\n",
    "os.makedirs(output_frames_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcs_subfolder = os.path.join(gcs_folder, 'classification_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_images_folder = os.path.join(gcs_subfolder, 'images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcs_cls_mainfest_path = os.path.join(gcs_subfolder, 'annotations.jsonl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Read manual annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 5 {manual_annotations_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(manual_annotations_path) as f:\n",
    "    manual_annotations = list(csv.DictReader(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cast numeric fields to float\n",
    "numeric_fields = ['bbox_x', 'bbox_y', 'bbox_width', 'bbox_height']\n",
    "\n",
    "for annotation in manual_annotations:\n",
    "    for col, value in annotation.items():\n",
    "        if col in numeric_fields:\n",
    "            annotation[col] = float(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train/test split for objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this subsection we divide all objects into 3 disjoint sets. This should be done to avoid data leaks during the model training and to be able to correctly evaluate the model quality. However for our demo we will ignore this split, because we only have one short video and do not have enough examples of anomalous objects to train the models properly (only 4 anomalous objects)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_annotations[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_objects = set()\n",
    "for annotation in manual_annotations:\n",
    "    all_objects.add(annotation['object_id'])\n",
    "len(all_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIXED_RANDOM_SEED = 10\n",
    "fixed_random = random.Random(FIXED_RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SIZE = 0.7\n",
    "VAL_SIZE = 0.15\n",
    "TEST_SIZE = 0.15\n",
    "\n",
    "assert TRAIN_SIZE + VAL_SIZE + TEST_SIZE == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_objects = list(all_objects)\n",
    "fixed_random.shuffle(all_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lim = int(round(len(all_objects) * TRAIN_SIZE))\n",
    "val_lim = int(round(len(all_objects) * (TRAIN_SIZE + VAL_SIZE)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_objects = set(all_objects[:train_lim])\n",
    "val_objects = set(all_objects[train_lim:val_lim])\n",
    "test_objects = set(all_objects[val_lim:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{} / {} / {}'.format(len(train_objects), len(val_objects), len(test_objects)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iterate over object frames and save them to separate images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_RATE = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_to_annotations = defaultdict(list)\n",
    "for manual_annotation in tqdm(manual_annotations):\n",
    "    image_name = manual_annotation['image_name'].split('.')[0]\n",
    "    obj_id = manual_annotation['object_id']\n",
    "    label = manual_annotation['class']\n",
    "    frame_id = int(image_name[len('frame_'):])\n",
    "    if frame_id % SAMPLE_RATE == 0:\n",
    "        image = read_image(image_name)\n",
    "        x = int(manual_annotation['bbox_x'])\n",
    "        y = int(manual_annotation['bbox_y'])\n",
    "        w = int(manual_annotation['bbox_width'])\n",
    "        h = int(manual_annotation['bbox_height'])\n",
    "        image_bbox = crop_n_resize_image(image, (x, y, w, h), (IMAGE_WIDTH, IMAGE_HEIGHT), PAD_COLOR)\n",
    "        save_name = f'{image_name}_{x}_{y}_{w}_{h}_{obj_id}_{label}.jpg'\n",
    "        save_path = os.path.join(cls_frames_dir, save_name)\n",
    "        cv2.imwrite(save_path, cv2.cvtColor(image_bbox, cv2.COLOR_RGB2BGR))\n",
    "        annotations_row = (save_name, save_path, label)\n",
    "        obj_to_annotations[obj_id].append(annotations_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = []\n",
    "for split_objects, split_name in zip((train_objects, val_objects, test_objects), ('train', 'validation', 'test')):\n",
    "    for idx in split_objects:\n",
    "        for annotations_row in obj_to_annotations[idx]:\n",
    "            annotations.append(annotations_row + (split_name, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(x[-2: ] for x in annotations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a manifest file in VertexAI format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc_annotations = []\n",
    "for image_name, image_path, class_label, data_split in annotations:\n",
    "    image_gcs_path = os.path.join('gs://', gcs_bucket, upload_images_folder, image_name)\n",
    "        \n",
    "    gc_annotations.append({\n",
    "        \"imageGcsUri\": image_gcs_path,\n",
    "        \"classificationAnnotation\": {\n",
    "            \"displayName\": class_label\n",
    "        },\n",
    "        \"dataItemResourceLabels\": {\n",
    "            \"aiplatform.googleapis.com/ml_use\": data_split\n",
    "        }\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manifest_lines = [json.dumps(annotation) + '\\n' for annotation in gc_annotations]\n",
    "with open(output_manifest_path, 'wt') as f:\n",
    "    f.writelines(manifest_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Upload images and manifest to gcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_client = storage.Client(project=gcloud_project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = storage_client.bucket(gcs_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_name, image_path, _, _ in tqdm(annotations):\n",
    "    destination_path = os.path.join(upload_images_folder, image_name)\n",
    "    blob = bucket.blob(destination_path)\n",
    "    blob.upload_from_filename(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bucket.blob(gcs_cls_mainfest_path).upload_from_filename(output_manifest_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and train VertexAI models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aiplatform.init(project=gcloud_project, location=gcloud_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Creating detection dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_dataset_name = 'detection_ds'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcs_detection_manifest_uri = os.path.join('gs://', gcs_bucket, gcs_detection_mainfest_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_dataset = aiplatform.ImageDataset.create(\n",
    "    display_name=detection_dataset_name,\n",
    "    gcs_source=[gcs_detection_manifest_uri],\n",
    "    import_schema_uri=aiplatform.schema.dataset.ioformat.image.bounding_box,\n",
    "    sync=True,\n",
    ")\n",
    "\n",
    "detection_dataset.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating anomaly classification dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_dataset_name = 'anomaly_classification_ds'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcs_cls_manifest_uri = os.path.join('gs://', gcs_bucket, gcs_cls_mainfest_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_dataset = aiplatform.ImageDataset.create(\n",
    "    display_name=cls_dataset_name,\n",
    "    gcs_source=[gcs_cls_manifest_uri],\n",
    "    import_schema_uri=aiplatform.schema.dataset.ioformat.image.single_label_classification,\n",
    "    sync=True,\n",
    ")\n",
    "\n",
    "cls_dataset.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Creating/training object detection model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_model_name = 'object_detection_model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_budget_milli_node_hours = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_job_name = f'{detection_model_name}_training_job'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_job = aiplatform.AutoMLImageTrainingJob(\n",
    "    display_name=cls_job_name,\n",
    "    model_type=\"CLOUD\",\n",
    "    prediction_type=\"object_detection\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_model = detection_job.run(\n",
    "    dataset=detection_dataset,\n",
    "    model_display_name=detection_model_name,\n",
    "    training_filter_split=\"labels.aiplatform.googleapis.com/ml_use=training\",\n",
    "    validation_filter_split=\"labels.aiplatform.googleapis.com/ml_use=validation\",\n",
    "    test_filter_split=\"labels.aiplatform.googleapis.com/ml_use=test\",\n",
    "    budget_milli_node_hours=detection_budget_milli_node_hours,\n",
    "    sync=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_type ='test'\n",
    "manifest_file = os.path.join(dataset_folder, 'annotations_test.manifest')\n",
    "\n",
    "print('Creating dataset...')\n",
    "dataset=json.loads('{ \"GroundTruthManifest\": { \"S3Object\": { \"Bucket\": \"' + bucket + '\", \"Key\": \"'+ manifest_file + '\" } } }')\n",
    "\n",
    "response=client.create_dataset(ProjectName=project, DatasetType=dataset_type, DatasetSource=dataset)\n",
    "print('Dataset Status: ' + response['DatasetMetadata']['Status'])\n",
    "print('Dataset Status Message: ' + response['DatasetMetadata']['StatusMessage'])\n",
    "print('Dataset Type: ' + response['DatasetMetadata']['DatasetType'])\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Creating/training anomaly classification model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models training will take several hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_model_name = 'anomaly_classification_model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_budget_milli_node_hours = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_job_name = f'{cls_model_name}_training_job'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_job = aiplatform.AutoMLImageTrainingJob(\n",
    "    display_name=cls_job_name,\n",
    "    model_type=\"CLOUD\",\n",
    "    prediction_type=\"classification\",\n",
    "    multi_label=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_model = cls_job.run(\n",
    "    dataset=cls_dataset,\n",
    "    model_display_name=cls_model_name,\n",
    "# as we don't have enough data, we will use random split instead for our demo scenario\n",
    "# (this may lead to the model overfitting)\n",
    "#    training_filter_split=\"labels.aiplatform.googleapis.com/ml_use=training\",\n",
    "#    validation_filter_split=\"labels.aiplatform.googleapis.com/ml_use=validation\",\n",
    "#    test_filter_split=\"labels.aiplatform.googleapis.com/ml_use=test\",\n",
    "    budget_milli_node_hours=cls_budget_milli_node_hours,\n",
    "    sync=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait until the both training jobs finish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_model.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_model.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Deploy vertexai models for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_ENDPOINT = \"us-central1-aiplatform.googleapis.com\"\n",
    "\n",
    "# The AI Platform services require regional API endpoints.\n",
    "client_options = {\"api_endpoint\": API_ENDPOINT}\n",
    "# Initialize client that will be used to create and send requests.\n",
    "# This client only needs to be created once, and can be reused for multiple requests.\n",
    "vertexai_client = aiplatform.gapic.PredictionServiceClient(client_options=client_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_endpoint_name = f'{gcloud_project}-detection-endpoint'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_endpoint = aiplatform.Endpoint.create(\n",
    "    display_name=detection_endpoint_name,\n",
    "    project=gcloud_project,\n",
    "    location=gcloud_location,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_model.deploy(detection_endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image_object_detection(\n",
    "    project: str,\n",
    "    endpoint_id: str,\n",
    "    image: np.ndarray,\n",
    "    vertexai_client,\n",
    "    location: str = \"us-central1\",\n",
    "    confidence_threshold: float = 0.0\n",
    "):\n",
    "    _, image_bytes = cv2.imencode('.jpg', image)\n",
    "\n",
    "    # The format of each instance should conform to the deployed model's prediction input schema.\n",
    "    encoded_content = base64.b64encode(image_bytes).decode(\"utf-8\")\n",
    "    instance = predict.instance.ImageObjectDetectionPredictionInstance(\n",
    "        content=encoded_content,\n",
    "    ).to_value()\n",
    "    instances = [instance]\n",
    "    # See gs://google-cloud-aiplatform/schema/predict/params/image_object_detection_1.0.0.yaml for the format of the parameters.\n",
    "    parameters = predict.params.ImageObjectDetectionPredictionParams(\n",
    "        confidence_threshold=confidence_threshold,\n",
    "    ).to_value()\n",
    "    endpoint = vertexai_client.endpoint_path(\n",
    "        project=project, location=location, endpoint=endpoint_id\n",
    "    )\n",
    "    response = vertexai_client.predict(\n",
    "        endpoint=endpoint, instances=instances, parameters=parameters\n",
    "    )\n",
    "    # See gs://google-cloud-aiplatform/schema/predict/prediction/image_object_detection_1.0.0.yaml for the format of the predictions.\n",
    "    predictions = response.predictions\n",
    "    return dict(predictions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anomaly classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_endpoint_name = f'{gcloud_project}-classification-endpoint'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_endpoint = aiplatform.Endpoint.create(\n",
    "    display_name=cls_endpoint_name,\n",
    "    project=gcloud_project,\n",
    "    location=gcloud_location,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_model.deploy(classification_endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image_classification(\n",
    "    project: str,\n",
    "    endpoint_id: str,\n",
    "    image: np.ndarray,\n",
    "    vertexai_client,\n",
    "    location: str = \"us-central1\"\n",
    "):\n",
    "    _, image_bytes = cv2.imencode('.jpg', image)\n",
    "\n",
    "    # The format of each instance should conform to the deployed model's prediction input schema.\n",
    "    encoded_content = base64.b64encode(image_bytes).decode(\"utf-8\")\n",
    "    instance = predict.instance.ImageClassificationPredictionInstance(\n",
    "        content=encoded_content,\n",
    "    ).to_value()\n",
    "    instances = [instance]\n",
    "    # See gs://google-cloud-aiplatform/schema/predict/params/image_classification_1.0.0.yaml for the format of the parameters.\n",
    "    parameters = predict.params.ImageClassificationPredictionParams(\n",
    "        confidence_threshold=0.5,\n",
    "    ).to_value()\n",
    "    endpoint = vertexai_client.endpoint_path(\n",
    "        project=project, location=location, endpoint=endpoint_id\n",
    "    )\n",
    "    response = vertexai_client.predict(\n",
    "        endpoint=endpoint, instances=instances, parameters=parameters\n",
    "    )\n",
    "    # gt.print_report()\n",
    "    \n",
    "    # See gs://google-cloud-aiplatform/schema/predict/prediction/image_classification_1.0.0.yaml for the format of the predictions.\n",
    "    predictions = response.predictions\n",
    "    return dict(predictions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vertexai prediction functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vertexai_get_bboxes(image):\n",
    "    preds = predict_image_object_detection(gcloud_project, detection_endpoint.name, image, \n",
    "                                           vertexai_client, confidence_threshold=0.001)\n",
    "    result = []\n",
    "    for xMin, xMax, yMin, yMax in preds['bboxes']:\n",
    "        frame_height, frame_width, _ = sample_frame.shape\n",
    "        x, y = xMin * frame_width, yMin * frame_height\n",
    "        w, h = (xMax - xMin) * frame_width, (yMax - yMin) * frame_height\n",
    "        result.append((x, y, w, h))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vertexai_classify_anomalies(image):\n",
    "    cls_preds = predict_image_classification(gcloud_project, classification_endpoint.name,\n",
    "                                             image, vertexai_client)\n",
    "    return cls_preds['displayNames'][0] != 'normal', cls_preds['confidences'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of predictions for a video frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_frame = read_image(frames[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_boxes = vertexai_get_bboxes(sample_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detection predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (20,20))\n",
    "ax.imshow(sample_frame)\n",
    "for x, y, w, h in _boxes:\n",
    "    rect = patches.Rectangle((x, y), w, h, linewidth=3, edgecolor='cyan', facecolor='none')\n",
    "    ax.add_patch(rect)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anomaly classification predictions: boolean flag is_anomaly and confidence score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for bbox in _boxes[0:2]:\n",
    "    bbox = tuple(map(int, bbox))\n",
    "    image_bbox = crop_n_resize_image(sample_frame, bbox, (IMAGE_WIDTH, IMAGE_HEIGHT), PAD_COLOR)\n",
    "    plt.imshow(image_bbox)\n",
    "    plt.show()\n",
    "    _pred = vertexai_classify_anomalies(image_bbox)\n",
    "    print(_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main step: do object tracking and anomaly detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AREA_INTERSECTION_THRESH = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_coordinates(rect):\n",
    "    '''\n",
    "    returns bottom left and top right corners coordinates of a rectangle\n",
    "    rect: rectangle in X, Y, W, H format\n",
    "    '''\n",
    "    x1, y1, w, h = rect\n",
    "    x2 = x1 + w\n",
    "    y2 = y1 + h\n",
    "    x1, x2 = min(x1, x2), max(x1, x2)\n",
    "    y1, y2 = min(y1, y2), max(y1, y2)\n",
    "    return x1, y1, x2, y2\n",
    "\n",
    "\n",
    "def intersect_rectangles(rect1, rect2):\n",
    "    '''\n",
    "    finds intersection of 2 rectangles as a rectangle or None, if they don't intersect\n",
    "    rect1, rect2: rectangles in X, Y, W, H format\n",
    "    '''\n",
    "    l1, d1, r1, u1 = _get_coordinates(rect1)\n",
    "    l2, d2, r2, u2 = _get_coordinates(rect2)\n",
    "    l = max(l1, l2)\n",
    "    r = min(r1, r2)\n",
    "    d = max(d1, d2)\n",
    "    u = min(u1, u2)\n",
    "    if l < r and d < u:\n",
    "        return l, d, r - l, u - d\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def get_area(rect):\n",
    "    '''\n",
    "    finds area of a rectangle\n",
    "    rect: a rectangle in X, Y, W, H format\n",
    "    '''\n",
    "    x, y, w, h = rect\n",
    "    return w * h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objects that touch left or right edge considered as partially visible (because in our scenario the \"conveyor belt\" moves from left to right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EDGE_WIDTH_PERCENT = 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EDGE_WIDTH_PERCENT - a width of left and right edge areas (in percentage of the total image width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def not_touches_edge(box, frame_shape):\n",
    "    '''\n",
    "    checks if a bounding box doesn't touch an image edge (left or right)\n",
    "    box: a bounding box in X, Y, W, H format\n",
    "    frame_shape: image size in H, W format\n",
    "    '''\n",
    "    x_min = frame_shape[1] * EDGE_WIDTH_PERCENT\n",
    "    x_max = frame_shape[1] - x_min\n",
    "    y_min = frame_shape[0] * EDGE_WIDTH_PERCENT\n",
    "    y_max = frame_shape[0] - y_min\n",
    "    x0, y0, w, h = box\n",
    "    x1 = x0 + w\n",
    "    y1 = y0 + h\n",
    "    return x_min <= x0 and x_min <= x1\n",
    "#     return x_min <= x0 < x_max and y_min <= y0 < y_max and x_min <= x1 < x_max and y_min <= y1 < y_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_anomaly_confidence(ad_predictions, object_id, num_preds=2):\n",
    "    preds = ad_predictions[object_id]\n",
    "    if len(preds) < num_preds:\n",
    "        return None, None\n",
    "    confidence = sum(preds) / len(preds)\n",
    "    if confidence > 0.5:\n",
    "        return True, confidence\n",
    "    else:\n",
    "        return False, 1 - confidence\n",
    "    \n",
    "def upd_predictions(ad_predictions, object_id, prediction, num_preds=2):\n",
    "    preds = ad_predictions[object_id]\n",
    "    is_anomaly, confidence = prediction\n",
    "    if not is_anomaly:\n",
    "        confidence = 1 - confidence\n",
    "    preds.append(confidence)\n",
    "    return get_anomaly_confidence(ad_predictions, object_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main pipeline steps:\n",
    "* iterate over frames, detect new objects and track existing ones\n",
    "    * iterate over objects and drop overlapping boxes (rarely happens because of false positives in tracking and detection algorithms)\n",
    "    * iterate over objects and assign anomaly classification labels (using the anomaly classification model for  new objects and caching the resulting labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downscale_ratio = 0.4\n",
    "\n",
    "tracker = MultiTracker(dst_thresh=DST_THRESHOLD * downscale_ratio, tracker_func=cv2.legacy.TrackerKCF_create,\n",
    "                        upd_area_thresh=1.15, upd_dst_thresh=UPD_DST_THRESHOLD * downscale_ratio)\n",
    "tracking_ad_results = {}\n",
    "\n",
    "ad_predictions = defaultdict(list)\n",
    "\n",
    "for frame_id, (frame_name, frame_path) in enumerate(tqdm(frames)):\n",
    "    frame = cv2.imread(frame_path)\n",
    "        \n",
    "    downscale_size = (int(frame.shape[1] * downscale_ratio), int(frame.shape[0] * downscale_ratio))\n",
    "    downscaled_frame = cv2.resize(frame, downscale_size)\n",
    "        \n",
    "    tracker.update(downscaled_frame)\n",
    "    \n",
    "    # do object detection to add new objects\n",
    "    boxes = vertexai_get_bboxes(frame)\n",
    "    downscaled_boxes = np.array(boxes) * downscale_ratio\n",
    "\n",
    "    tracker.add_boxes(downscaled_boxes, downscaled_frame)\n",
    "    \n",
    "    boxes = [(idx, (np.array(box) / downscale_ratio)) for idx, box in tracker.get_objects()]\n",
    "    \n",
    "    \n",
    "    # filter overlapping boxes\n",
    "    overlapped_boxes = set()\n",
    "    for idx1, box1 in boxes:\n",
    "        for idx2, box2 in boxes:\n",
    "            if idx1 != idx2:\n",
    "                intersection = intersect_rectangles(box1, box2)\n",
    "                if intersection is not None and \\\n",
    "                        get_area(intersection) >= min(get_area(box1), get_area(box2)) * AREA_INTERSECTION_THRESH:\n",
    "                    if get_area(box1) < get_area(box2):\n",
    "                        overlapped_boxes.add(idx1)\n",
    "                    else:\n",
    "                        overlapped_boxes.add(idx2)\n",
    "    boxes = [(idx, box) for idx, box in boxes if idx not in overlapped_boxes]\n",
    "    \n",
    "    # add anomaly detection results\n",
    "    result = []\n",
    "    for idx, bbox in boxes:\n",
    "        is_anomaly, confidence = get_anomaly_confidence(ad_predictions, idx)\n",
    "        if is_anomaly is None or confidence < 0.9:\n",
    "            if not_touches_edge(bbox, frame.shape):\n",
    "                bbox = tuple(map(int, bbox))\n",
    "                image_bbox = crop_n_resize_image(frame, bbox, (IMAGE_WIDTH, IMAGE_HEIGHT), PAD_COLOR)\n",
    "                pred = vertexai_classify_anomalies(image_bbox)\n",
    "                is_anomaly, confidence = upd_predictions(ad_predictions, idx, pred)\n",
    "            else:\n",
    "                # object is partially visible, do not use anomaly classification for this frame\n",
    "                # (will be done later, when objects fully enter the frame)\n",
    "                is_anomaly, confidence = False, None\n",
    "        result.append((idx, bbox, is_anomaly, confidence))\n",
    "        \n",
    "    tracking_ad_results[frame_name] = result\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup vertexai resources (to save costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for model in detection_endpoint.list_models():\n",
    "    detection_endpoint.undeploy(model.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "detection_endpoint.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in classification_endpoint.list_models():\n",
    "    classification_endpoint.undeploy(model.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_endpoint.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the resulting frames and make a demo video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_frames = make_frames_with_tracking_boxes(frames, tracking_ad_results, output_result_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_video_from_frames(result_frames, result_video_path, fps=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final cleanup: delete all created resources from Google Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_model.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_model.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_dataset.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_dataset.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_client = storage.Client(project=gcloud_project)\n",
    "\n",
    "bucket = storage_client.bucket(gcs_bucket)\n",
    "bucket.delete()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv_anomaly",
   "language": "python",
   "name": "cv_anomaly"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
